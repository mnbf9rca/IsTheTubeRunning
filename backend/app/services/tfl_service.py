"""TfL Service for fetching and caching Transport for London data.

This module provides our internal service layer for interacting with Transport for London's
official API. We use the pydantic-tfl-api library (3rd party) to communicate with the real
TfL API, and this service wraps that library with caching, error handling, and database storage.

Terminology:
- "TfL Service" or "TfL Integration" = This module (our internal code)
- "TfL API" = Transport for London's official REST API
- "pydantic-tfl-api" = The 3rd party client library we use (authoritative for API schemas)
"""

import contextlib
import hashlib
import json
import uuid
from collections.abc import Generator
from datetime import UTC, datetime
from typing import Any, cast
from urllib.parse import urlparse

import structlog
from aiocache import Cache
from aiocache.serializers import PickleSerializer
from fastapi import HTTPException, status
from opentelemetry import trace
from pydantic_tfl_api import AsyncLineClient, AsyncStopPointClient
from pydantic_tfl_api.core import ApiError, ResponseModel
from pydantic_tfl_api.models import (
    DisruptedPoint,
    Disruption,
    LineStatus,
    MatchedStop,
    RouteSection,
    RouteSectionNaptanEntrySequence,
    RouteSequence,
    StopPoint,
)
from pydantic_tfl_api.models import (
    Line as TflLine,
)
from redis.exceptions import RedisError
from sqlalchemy import delete, func, or_, select
from sqlalchemy.dialects.postgresql import insert
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import aliased

from app.core.config import settings
from app.core.telemetry import get_current_trace_id
from app.helpers.route_validation import find_valid_connection_in_routes
from app.helpers.soft_delete_filters import add_active_filter, soft_delete
from app.helpers.station_fetching import (
    DatabaseNotInitializedError,
    LineNotFoundError,
    NoStationsForLineError,
    build_station_cache_key,
    filter_stations_by_line_tfl_id,
    is_database_initialized,
    validate_stations_exist_for_line,
)
from app.helpers.station_resolution import (
    NoMatchingStationsError,
    StationNotFoundError,
    create_hub_representative,
    filter_stations_by_line,
    group_stations_by_hub,
    select_station_from_candidates,
)
from app.models.tfl import (
    AlertDisabledSeverity,
    DisruptionCategory,
    Line,
    LineChangeLog,
    SeverityCode,
    Station,
    StationConnection,
    StationDisruption,
    StopType,
)
from app.schemas.tfl import (
    AffectedRouteInfo,
    DisruptionResponse,
    LineRoutesResponse,
    RouteSegmentRequest,
    RouteVariant,
    StationDisruptionResponse,
    StationRouteInfo,
    StationRoutesResponse,
)
from app.types.tfl_api import NetworkConnection

## Note: This code contains a mix of application-internal schemas and
## schemas from the pydantic-tfl-api package.
## Because the pydantic-tfl-api schemas are autogenerated from the TfL
## OpenAPI spec, they are considered authoritative.
## Where there is a mismatch or inconsistency, the pydantic-tfl-api
## package is the source of truth for TfL API data structures.

logger = structlog.get_logger(__name__)


# Custom domain exceptions


class MetadataChangeDetectedError(Exception):
    """
    Raised when TfL metadata changes unexpectedly during refresh.

    This indicates that TfL API data has changed, which should be rare for
    "super static" data like severity codes, disruption categories, and stop types.
    When this occurs, it may indicate:
    - TfL API schema changes (needs code update)
    - Unexpected TfL behavior (needs investigation)
    - Cache/persistence logic issues

    The error message includes details about what changed for debugging.
    """

    def __init__(
        self,
        message: str,
        details: dict[str, Any] | None = None,
        before_counts: tuple[int, int, int] | None = None,
        after_counts: tuple[int, int, int] | None = None,
    ) -> None:
        """
        Initialize metadata change error.

        Args:
            message: Human-readable error description
            details: Optional dict with change details (old/new hashes, counts, etc.)
            before_counts: Tuple of (severity_codes, categories, stop_types) before refresh
            after_counts: Tuple of (severity_codes, categories, stop_types) after refresh
        """
        super().__init__(message)
        self.details = details or {}
        self.before_counts = before_counts
        self.after_counts = after_counts


def _compute_metadata_hash(
    items: list[SeverityCode] | list[DisruptionCategory] | list[StopType],
) -> str:
    """
    Compute stable SHA256 hash of metadata collection.

    This pure function creates a deterministic hash by:
    1. Sorting items by their unique identifier fields
    2. Serializing to JSON with sorted keys
    3. Computing SHA256 hash

    Used for change detection - if hash changes, metadata has changed.

    Args:
        items: List of metadata objects (all must be same type)

    Returns:
        Hex digest of SHA256 hash

    Examples:
        >>> codes = [SeverityCode(mode_id="tube", severity_level=10, ...)]
        >>> hash1 = _compute_metadata_hash(codes)
        >>> hash2 = _compute_metadata_hash(codes)  # Same data
        >>> assert hash1 == hash2  # Stable hashing
    """
    # Sort items to ensure stable ordering
    if not items:
        return hashlib.sha256(b"[]").hexdigest()

    # Determine item type and extract sortable data
    if isinstance(items[0], SeverityCode):
        # Sort by mode_id, severity_level
        data = [
            {
                "mode_id": item.mode_id,
                "severity_level": item.severity_level,
                "description": item.description,
            }
            for item in sorted(
                cast(list[SeverityCode], items),
                key=lambda x: (x.mode_id, x.severity_level),
            )
        ]
    elif isinstance(items[0], DisruptionCategory):
        # Sort by category_name
        data = [
            {"category_name": item.category_name, "description": item.description}
            for item in sorted(
                cast(list[DisruptionCategory], items),
                key=lambda x: x.category_name,
            )
        ]
    elif isinstance(items[0], StopType):
        # Sort by type_name
        data = [
            {"type_name": item.type_name, "description": item.description}
            for item in sorted(
                cast(list[StopType], items),
                key=lambda x: x.type_name,
            )
        ]
    else:
        msg = f"Unsupported metadata type: {type(items[0])}"
        raise TypeError(msg)

    # Serialize to JSON with sorted keys for stability
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=True)

    # Compute SHA256 hash
    return hashlib.sha256(json_str.encode("utf-8")).hexdigest()


@contextlib.contextmanager
def tfl_api_span(
    endpoint: str,
    client: str,
    **extra_attrs: str | int,
) -> Generator[trace.Span]:
    """Context manager for TfL API call spans with standard attributes.

    Creates an OpenTelemetry span with consistent attributes for TfL API calls.
    The SDK automatically records exceptions and sets error status if an
    exception occurs within the span context.

    Note: The tracer is acquired at call time (not module import time) to ensure
    it uses the TracerProvider set during application startup. This enables proper
    trace context propagation from parent spans (e.g., FastAPI HTTP requests).

    Args:
        endpoint: API endpoint name (e.g., "MetaModes", "GetByModeByPathModes")
        client: Client type ("line_client" or "stoppoint_client")
        **extra_attrs: Additional span attributes (e.g., tfl.api.mode, tfl.api.line_id)

    Yields:
        Span: The active span for setting additional attributes like http.status_code

    Example:
        with tfl_api_span("MetaModes", "line_client") as span:
            response = await self.line_client.MetaModes()
            if hasattr(response, "http_status_code"):
                span.set_attribute("http.status_code", response.http_status_code)
    """
    # Get tracer at call time to use the correct TracerProvider (set in FastAPI lifespan)
    tracer = trace.get_tracer(__name__)
    attributes = {
        "tfl.api.endpoint": endpoint,
        "tfl.api.client": client,
        "peer.service": "api.tfl.gov.uk",
        **extra_attrs,
    }
    with tracer.start_as_current_span(
        f"tfl.api.{endpoint}",
        kind=trace.SpanKind.CLIENT,
        attributes=attributes,
    ) as span:
        yield span


# Default cache TTL values (in seconds) - used if TfL API doesn't provide cache headers
DEFAULT_LINES_CACHE_TTL = 86400  # 24 hours
DEFAULT_STATIONS_CACHE_TTL = 86400  # 24 hours
DEFAULT_DISRUPTIONS_CACHE_TTL = 120  # 2 minutes
DEFAULT_METADATA_CACHE_TTL = 86400  # 24 hours (matches typical TfL API expiry)

# TfL API constants
MIN_ROUTE_SEGMENTS = 2  # Minimum number of segments required for route validation
MAX_ROUTE_SEGMENTS = 20  # Maximum number of segments allowed for route validation
DEFAULT_MODES = ["tube", "overground", "dlr", "elizabeth-line"]  # Default transport modes to fetch


# Pure helper functions for station disruption processing


def _generate_station_disruption_tfl_id(
    station_atco_code: str,
    from_date: datetime,
    to_date: datetime | None,
    description: str,
) -> str:
    """Generate stable hash-based TfL ID for station disruption.

    Uses SHA-256 hash of key fields to create a deterministic identifier that:
    - Remains stable across API fetches (same disruption = same ID)
    - Uniquely identifies each disruption (different data = different ID)
    - Enables database deduplication via unique constraint

    Args:
        station_atco_code: ATCO code of affected station (e.g., '940GZZLUEUS')
        from_date: Disruption start date from TfL API
        to_date: Disruption end date from TfL API (may be None for indefinite)
        description: Full disruption description text

    Returns:
        16-character hexadecimal hash string (first 16 chars of SHA-256)

    Example:
        >>> from datetime import datetime, UTC
        >>> _generate_station_disruption_tfl_id(
        ...     "940GZZLUEUS",
        ...     datetime(2025, 11, 14, 10, 0, tzinfo=UTC),
        ...     datetime(2025, 11, 15, 18, 0, tzinfo=UTC),
        ...     "Lift out of service"
        ... )
        'a1b2c3d4e5f67890'
    """
    # Build stable content string with pipe delimiters
    to_date_str = to_date.isoformat() if to_date else "no-end-date"
    content = f"{station_atco_code}|{from_date.isoformat()}|{to_date_str}|{description}"

    # Generate SHA-256 hash and take first 16 characters for compact ID
    hash_bytes = hashlib.sha256(content.encode("utf-8")).hexdigest()
    return hash_bytes[:16]


def _parse_tfl_timestamp(timestamp_str: str | None) -> datetime | None:
    """Parse TfL API timestamp string to timezone-aware datetime.

    TfL API returns ISO 8601 timestamps with 'Z' suffix (UTC).
    Example: '2025-11-14T10:30:00Z'

    Args:
        timestamp_str: ISO 8601 timestamp string from TfL API, or None

    Returns:
        Parsed datetime with UTC timezone, or None if input is invalid/missing

    Example:
        >>> _parse_tfl_timestamp("2025-11-14T10:30:00Z")
        datetime(2025, 11, 14, 10, 30, 0, tzinfo=UTC)
        >>> _parse_tfl_timestamp(None)
        None
        >>> _parse_tfl_timestamp("invalid")
        None
    """
    if not timestamp_str:
        return None

    try:
        # Convert 'Z' suffix to '+00:00' for ISO 8601 parsing
        normalized = timestamp_str.replace("Z", "+00:00")
        return datetime.fromisoformat(normalized)
    except (ValueError, AttributeError):
        logger.debug("failed_to_parse_tfl_timestamp", timestamp=timestamp_str)
        return None


def _extract_station_atco_code(disrupted_point: DisruptedPoint) -> str | None:
    """Extract ATCO code from TfL API DisruptedPoint object.

    Tries stationAtcoCode first (more explicit), falls back to atcoCode.
    Both fields identify the same station using NaPTAN ATCO code standard.

    Args:
        disrupted_point: DisruptedPoint object from TfL API

    Returns:
        ATCO code string (e.g., '940GZZLUEUS'), or None if both fields missing

    Example:
        >>> point = DisruptedPoint(stationAtcoCode="940GZZLUEUS", ...)
        >>> _extract_station_atco_code(point)
        '940GZZLUEUS'
    """
    # stationAtcoCode is more explicit, use it if available
    station_code: str | None = getattr(disrupted_point, "stationAtcoCode", None)
    if station_code:
        return station_code

    # Fall back to atcoCode (less specific but often same value)
    atco_code: str | None = getattr(disrupted_point, "atcoCode", None)
    return atco_code


async def _cache_metadata_items(
    cache: Cache,
    cache_key: str,
    items: list[Any],
    ttl: int,
    metadata_type: str,
) -> None:
    """
    Helper function to cache metadata items with logging.

    Args:
        cache: Cache instance to use
        cache_key: Redis key to store items under
        items: List of items to cache
        ttl: Time-to-live in seconds
        metadata_type: Type name for logging (e.g., "severity_codes")
    """
    if items:
        await cache.set(cache_key, items, ttl=ttl)
        logger.info(
            f"{metadata_type}_cache_warmed",
            count=len(items),
            ttl=ttl,
        )
    else:
        logger.warning(f"{metadata_type}_table_empty_skipping_cache_warmup")


async def warm_up_metadata_cache(db: AsyncSession, redis_url: str) -> dict[str, int]:
    """
    Warm up Redis cache with metadata from database on application startup.

    Called during application lifespan to rehydrate Redis from PostgreSQL,
    eliminating cold-start dependency on TfL API. If database tables are empty
    (fresh installation), returns zero counts and logs warnings.

    Args:
        db: Database session for querying metadata tables
        redis_url: Redis connection URL for cache initialization

    Returns:
        dict with counts: {
            "severity_codes_count": int,
            "disruption_categories_count": int,
            "stop_types_count": int
        }

    Raises:
        Does not raise exceptions - logs warnings for graceful degradation.
        Application startup continues even if warm-up fails.

    Example:
        >>> async with get_session_factory()() as session:
        ...     counts = await warm_up_metadata_cache(session, "redis://localhost:6379/0")
        ...     print(counts)
        {"severity_codes_count": 233, "disruption_categories_count": 7, "stop_types_count": 3}
    """
    try:
        # Query all metadata from database
        severity_result = await db.execute(select(SeverityCode))
        severity_codes = list(severity_result.scalars().all())

        disruption_result = await db.execute(select(DisruptionCategory))
        disruption_categories = list(disruption_result.scalars().all())

        stop_result = await db.execute(select(StopType))
        stop_types = list(stop_result.scalars().all())

        # Initialize cache (same configuration as TfLService)
        # Parse Redis URL to extract host and port
        parsed = urlparse(redis_url)
        redis_host = parsed.hostname or "localhost"
        redis_port = parsed.port or 6379

        cache = Cache(
            Cache.REDIS,
            endpoint=redis_host,
            port=redis_port,
            serializer=PickleSerializer(),
            namespace="tfl",
        )

        try:
            # Populate Redis with same keys used by fetch_* methods
            # Use DEFAULT_METADATA_CACHE_TTL (24 hours) matching typical TfL API expiry
            ttl = DEFAULT_METADATA_CACHE_TTL

            # Cache each metadata type using helper function
            await _cache_metadata_items(cache, "severity_codes:all", severity_codes, ttl, "severity_codes")
            await _cache_metadata_items(
                cache, "disruption_categories:all", disruption_categories, ttl, "disruption_categories"
            )
            await _cache_metadata_items(cache, "stop_types:all", stop_types, ttl, "stop_types")

            return {
                "severity_codes_count": len(severity_codes),
                "disruption_categories_count": len(disruption_categories),
                "stop_types_count": len(stop_types),
            }
        finally:
            await cache.close()

    except (SQLAlchemyError, RedisError) as exc:
        # Log error but don't block application startup
        # Catches database errors (SQLAlchemyError) and Redis errors (RedisError)
        logger.error(
            "metadata_cache_warmup_failed",
            error=str(exc),
            error_type=type(exc).__name__,
            exc_info=exc,
        )
        # Return zero counts to indicate failure
        return {
            "severity_codes_count": 0,
            "disruption_categories_count": 0,
            "stop_types_count": 0,
        }


def _normalize_route_variants(routes: list[list[str]]) -> dict[str, Any]:
    """Sort routes for deterministic JSON storage.

    The TfL API may return route variants in any order.
    Sorting ensures the same routes produce the same JSON,
    avoiding false-positive change detection.

    Args:
        routes: List of route variants, each a list of station IDs

    Returns:
        Normalized {"routes": [...]} dict with sorted outer list
    """
    sorted_routes = sorted(routes, key=lambda r: tuple(r))
    return {"routes": sorted_routes}


def _log_line_change_if_needed(
    db: AsyncSession,
    tfl_id: str | None,
    new_name: str | None,
    new_mode: str,
    existing_line: Line | None,
    detected_at: datetime,
) -> None:
    """Detect and log line metadata changes.

    Args:
        db: Database session to add log entry
        tfl_id: TfL line ID (None values are skipped)
        new_name: New line name from API (None values are skipped)
        new_mode: New line mode from API
        existing_line: Existing line in database, or None if creating
        detected_at: Timestamp when change was detected
    """
    # Skip if essential fields are missing
    if tfl_id is None or new_name is None:
        return

    new_values = {"name": new_name, "mode": new_mode}

    if existing_line is None:
        # New line created
        db.add(
            LineChangeLog(
                tfl_id=tfl_id,
                change_type="created",
                changed_fields=["name", "mode"],
                old_values=None,
                new_values=new_values,
                detected_at=detected_at,
                trace_id=get_current_trace_id(),
            )
        )
        return

    # Check for changes in existing line
    changed_fields = []
    old_values = {}

    if existing_line.name != new_name:
        changed_fields.append("name")
        old_values["name"] = existing_line.name

    if existing_line.mode != new_mode:
        changed_fields.append("mode")
        old_values["mode"] = existing_line.mode

    if changed_fields:
        db.add(
            LineChangeLog(
                tfl_id=tfl_id,
                change_type="updated",
                changed_fields=changed_fields,
                old_values=old_values,
                new_values={k: new_values[k] for k in changed_fields},
                detected_at=detected_at,
                trace_id=get_current_trace_id(),
            )
        )


class TfLService:
    """Service for interacting with Transport for London's API and managing transport data.

    This service uses the pydantic-tfl-api library to fetch data from the real TfL API,
    then caches responses in Redis and stores reference data in PostgreSQL.
    """

    def __init__(self, db: AsyncSession) -> None:
        """
        Initialize the TfL service.

        Args:
            db: Database session
        """
        self.db = db
        # pydantic-tfl-api v3 provides native async clients
        self.line_client = AsyncLineClient(api_token=settings.TFL_API_KEY)
        self.stoppoint_client = AsyncStopPointClient(api_token=settings.TFL_API_KEY)

        # Initialize Redis cache for aiocache
        self.cache = Cache(
            Cache.REDIS,
            endpoint=self._parse_redis_host(),
            port=self._parse_redis_port(),
            serializer=PickleSerializer(),
            namespace="tfl",
        )

    def _parse_redis_host(self) -> str:
        """Extract Redis host from REDIS_URL."""
        # Format: redis://host:port/db or redis://host:port
        parsed = urlparse(settings.REDIS_URL)
        return parsed.hostname or "localhost"

    def _parse_redis_port(self) -> int:
        """Extract Redis port from REDIS_URL."""
        # Format: redis://host:port/db or redis://host:port
        parsed = urlparse(settings.REDIS_URL)
        return parsed.port or 6379

    def _handle_api_error(self, response: ResponseModel[Any] | ApiError) -> None:
        """
        Check if TfL API response is an error and raise HTTPException.

        Args:
            response: TfL API response object or ApiError

        Raises:
            HTTPException: If response is an ApiError
        """
        if isinstance(response, ApiError):
            error_msg = f"TfL API error: {response.message}"
            logger.error("tfl_api_error", message=response.message, status=response.http_status_code)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=error_msg,
            )

    def _extract_cache_ttl(self, response: ResponseModel[Any]) -> int:
        """
        Extract cache TTL from TfL API response.

        Uses shared_expires (s-maxage) for shared caches like Redis, falling back to
        content_expires (max-age) if shared_expires is not available.

        Args:
            response: TfL API response object

        Returns:
            TTL in seconds from shared_expires/content_expires, or 0 if not available
        """
        # Prefer shared_expires (s-maxage) for shared caches like Redis
        expires = None
        if hasattr(response, "shared_expires") and response.shared_expires:
            expires = response.shared_expires
            logger.debug("extracted_cache_ttl", source="shared_expires")
        elif hasattr(response, "content_expires") and response.content_expires:
            expires = response.content_expires
            logger.debug("extracted_cache_ttl", source="content_expires")

        if expires:
            ttl = int((expires - datetime.now(UTC)).total_seconds())
            if ttl > 0:
                logger.debug("cache_ttl_calculated", ttl=ttl)
                return ttl

        return 0  # Return 0 to indicate no TTL found

    def _build_modes_cache_key(self, prefix: str, modes: list[str]) -> str:
        """
        Build a cache key for multi-mode endpoints.

        Args:
            prefix: Cache key prefix (e.g., "lines", "line_disruptions")
            modes: List of transport modes

        Returns:
            Formatted cache key with sorted modes
        """
        sorted_modes = ",".join(sorted(modes))
        return f"{prefix}:modes:{sorted_modes}"

    async def fetch_available_modes(self, use_cache: bool = True) -> list[str]:
        """
        Fetch all available transport modes from TfL API.

        Uses the MetaModes endpoint to dynamically discover all transport modes
        available in the TfL network (e.g., tube, overground, dlr, elizabeth-line).

        Args:
            use_cache: Whether to use Redis cache (default: True)

        Returns:
            List of mode strings (e.g., ["tube", "overground", "dlr"])
        """
        cache_key = "modes:all"

        # Try cache first
        if use_cache:
            cached_modes: list[str] | None = await self.cache.get(cache_key)
            if cached_modes is not None:
                logger.debug("modes_cache_hit", count=len(cached_modes))
                return cached_modes

        logger.info("fetching_modes_from_tfl_api")

        try:
            # Fetch from TfL API
            with tfl_api_span("MetaModes", "line_client") as span:
                response = await self.line_client.MetaModes()
                if hasattr(response, "http_status_code"):
                    span.set_attribute("http.status_code", response.http_status_code)

            # Check for API error
            self._handle_api_error(response)
            assert not isinstance(response, ApiError)  # Type narrowing for mypy

            # Extract modes from response
            # response.content is ModeArray (RootModel[list[Mode]]), access via .root
            # Then extract modeName from each Mode object
            modes: list[str] = [mode.modeName for mode in response.content.root if mode.modeName]

            # Use default cache TTL for metadata (7 days)
            # Modes don't change frequently
            ttl = DEFAULT_METADATA_CACHE_TTL

            # Cache the results
            await self.cache.set(cache_key, modes, ttl=ttl)

            logger.info("modes_fetched_and_cached", count=len(modes), ttl=ttl, modes=modes)
            return modes

        except Exception as e:
            logger.error("failed_to_fetch_modes", error=str(e))
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Failed to fetch transport modes: {e!s}",
            ) from e

    async def fetch_lines(
        self,
        modes: list[str] | None = None,
        use_cache: bool = True,
    ) -> list[Line]:
        """
        Fetch lines from TfL API for specified transport modes.

        Args:
            modes: List of transport modes to fetch (e.g., ["tube", "overground", "dlr"]).
                   If None, defaults to ["tube", "overground", "dlr", "elizabeth-line"].
            use_cache: Whether to use Redis cache (default: True)

        Returns:
            List of Line objects from database
        """
        # Default to major transport modes if not specified
        if modes is None:
            modes = DEFAULT_MODES

        cache_key = self._build_modes_cache_key("lines", modes)

        # Try cache first
        if use_cache:
            cached_lines: list[Line] | None = await self.cache.get(cache_key)
            if cached_lines is not None:
                logger.debug("lines_cache_hit", count=len(cached_lines), modes=modes)
                return cached_lines

        logger.info("fetching_lines_from_tfl_api", modes=modes)

        try:
            # Fetch lines for each mode and upsert (no delete!)
            all_lines = []
            ttl = DEFAULT_LINES_CACHE_TTL

            for mode in modes:
                logger.debug("fetching_lines_for_mode", mode=mode)

                # Fetch from TfL API
                with tfl_api_span(
                    "GetByModeByPathModes",
                    "line_client",
                    **{"tfl.api.mode": mode},
                ) as span:
                    response = await self.line_client.GetByModeByPathModes(mode)
                    if hasattr(response, "http_status_code"):
                        span.set_attribute("http.status_code", response.http_status_code)

                # Check for API error
                self._handle_api_error(response)
                assert not isinstance(response, ApiError)  # Type narrowing for mypy

                # Extract cache TTL from response (use minimum TTL across all modes)
                mode_ttl = self._extract_cache_ttl(response) or DEFAULT_LINES_CACHE_TTL
                ttl = min(ttl, mode_ttl)

                # Process lines for this mode
                # response.content is a LineArray (RootModel), access via .root
                line_data_list = response.content.root

                for line_data in line_data_list:
                    now = datetime.now(UTC)

                    # Check for existing line to detect changes
                    existing_result = await self.db.execute(select(Line).where(Line.tfl_id == line_data.id))
                    existing_line = existing_result.scalar_one_or_none()

                    # Detect changes and log (extracted to helper for complexity)
                    _log_line_change_if_needed(
                        db=self.db,
                        tfl_id=line_data.id,
                        new_name=line_data.name,
                        new_mode=mode,
                        existing_line=existing_line,
                        detected_at=now,
                    )

                    # Upsert the line (PostgreSQL INSERT ... ON CONFLICT DO UPDATE)
                    stmt = insert(Line).values(
                        tfl_id=line_data.id,
                        name=line_data.name,
                        mode=mode,
                        last_updated=now,
                    )
                    stmt = stmt.on_conflict_do_update(
                        index_elements=["tfl_id"],
                        set_={
                            "name": stmt.excluded.name,
                            "mode": stmt.excluded.mode,
                            "last_updated": stmt.excluded.last_updated,
                        },
                    )
                    await self.db.execute(stmt)

                    # Collect line for return (query back after upsert)
                    line_result = await self.db.execute(select(Line).where(Line.tfl_id == line_data.id))
                    all_lines.append(line_result.scalar_one())

                logger.debug("mode_lines_processed", mode=mode, count=len(line_data_list))

            await self.db.commit()

            # Refresh all lines to get latest data after commit
            for line in all_lines:
                await self.db.refresh(line)

            # Cache the results
            await self.cache.set(cache_key, all_lines, ttl=ttl)

            logger.info(
                "lines_fetched_and_cached",
                count=len(all_lines),
                modes=modes,
                ttl=ttl,
            )
            return all_lines

        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_lines_failed", error=str(e), modes=modes, exc_info=e)
            await self.db.rollback()
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Failed to fetch lines from TfL API for modes: {modes}",
            ) from e

    async def fetch_severity_codes(self, use_cache: bool = True) -> list[SeverityCode]:
        """
        Fetch severity codes metadata from TfL API.

        The TfL API returns severity codes with mode information (e.g., tube, dlr).
        Each mode has its own set of severity levels.

        Args:
            use_cache: Whether to use Redis cache (default: True)

        Returns:
            List of SeverityCode objects from database
        """
        cache_key = "severity_codes:all"

        # Try cache first
        if use_cache:
            cached_codes: list[SeverityCode] | None = await self.cache.get(cache_key)
            if cached_codes is not None:
                logger.debug("severity_codes_cache_hit", count=len(cached_codes))
                return cached_codes

        logger.info("fetching_severity_codes_from_tfl_api")

        try:
            # Fetch from TfL API
            with tfl_api_span("MetaSeverity", "line_client") as span:
                response = await self.line_client.MetaSeverity()
                if hasattr(response, "http_status_code"):
                    span.set_attribute("http.status_code", response.http_status_code)

            # Check for API error
            self._handle_api_error(response)
            assert not isinstance(response, ApiError)  # Type narrowing for mypy

            # Extract cache TTL from response
            ttl = self._extract_cache_ttl(response) or DEFAULT_METADATA_CACHE_TTL

            # Process and upsert severity codes (avoids race conditions)
            # response.content is a StatusSeveritiesArray (RootModel), access via .root
            severity_data_list = response.content.root

            now = datetime.now(UTC)
            for severity_data in severity_data_list:
                # Skip entries with missing required fields
                if severity_data.modeName is None or severity_data.severityLevel is None:
                    logger.warning(
                        "skipping_severity_code_missing_fields",
                        mode_name=severity_data.modeName,
                        severity_level=severity_data.severityLevel,
                    )
                    continue

                # Use PostgreSQL INSERT ... ON CONFLICT to atomically upsert
                # Now keyed on (mode_id, severity_level)
                stmt = insert(SeverityCode).values(
                    mode_id=severity_data.modeName,
                    severity_level=severity_data.severityLevel,
                    description=severity_data.description or "",
                    last_updated=now,
                )
                stmt = stmt.on_conflict_do_update(
                    constraint="uq_severity_code_mode_level",
                    set_={
                        "description": stmt.excluded.description,
                        "last_updated": stmt.excluded.last_updated,
                    },
                )
                await self.db.execute(stmt)

            # DEBUG: Log before commit
            logger.debug("severity_codes_committing_to_database", severity_count=len(severity_data_list))
            await self.db.commit()
            logger.info("severity_codes_committed_to_database", severity_count=len(severity_data_list))

            # Fetch all codes from database to return
            result = await self.db.execute(select(SeverityCode))
            codes = list(result.scalars().all())
            logger.debug("severity_codes_queried_from_database_after_commit", query_result_count=len(codes))

            # Cache the results
            await self.cache.set(cache_key, codes, ttl=ttl)

            logger.info("severity_codes_fetched_and_cached", count=len(codes), ttl=ttl)
            return codes

        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_severity_codes_failed", error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to fetch severity codes from TfL API.",
            ) from e

    async def get_alert_config(self) -> list[dict[str, Any]]:
        """
        Get the alert configuration showing which severities trigger alerts.

        Returns a list of all severity codes with their alert status.

        Returns:
            List of dicts with mode_id, severity_level, description, alerts_enabled
        """
        # Fetch all severity codes
        result = await self.db.execute(select(SeverityCode).order_by(SeverityCode.mode_id, SeverityCode.severity_level))
        severity_codes = result.scalars().all()

        # Fetch all disabled severities
        disabled_result = await self.db.execute(select(AlertDisabledSeverity))
        disabled_severities = {(d.mode_id, d.severity_level) for d in disabled_result.scalars().all()}

        # Build the response
        return [
            {
                "mode_id": code.mode_id,
                "severity_level": code.severity_level,
                "description": code.description,
                "alerts_enabled": (code.mode_id, code.severity_level) not in disabled_severities,
            }
            for code in severity_codes
        ]

    async def fetch_disruption_categories(self, use_cache: bool = True) -> list[DisruptionCategory]:
        """
        Fetch disruption categories metadata from TfL API.

        Args:
            use_cache: Whether to use Redis cache (default: True)

        Returns:
            List of DisruptionCategory objects from database
        """
        cache_key = "disruption_categories:all"

        # Try cache first
        if use_cache:
            cached_categories: list[DisruptionCategory] | None = await self.cache.get(cache_key)
            if cached_categories is not None:
                logger.debug("disruption_categories_cache_hit", count=len(cached_categories))
                return cached_categories

        logger.info("fetching_disruption_categories_from_tfl_api")

        try:
            # Fetch from TfL API
            with tfl_api_span("MetaDisruptionCategories", "line_client") as span:
                response = await self.line_client.MetaDisruptionCategories()
                if hasattr(response, "http_status_code"):
                    span.set_attribute("http.status_code", response.http_status_code)

            # Check for API error
            self._handle_api_error(response)
            assert not isinstance(response, ApiError)  # Type narrowing for mypy

            # Extract cache TTL from response
            ttl = self._extract_cache_ttl(response) or DEFAULT_METADATA_CACHE_TTL

            # Process and upsert disruption categories (avoids race conditions)
            # response.content is a RootModel array, access via .root
            category_data_list = response.content.root

            now = datetime.now(UTC)
            for category_data in category_data_list:
                # Use PostgreSQL INSERT ... ON CONFLICT to atomically upsert
                stmt = insert(DisruptionCategory).values(
                    category_name=category_data,
                    description=None,  # API only provides category name
                    last_updated=now,
                )
                stmt = stmt.on_conflict_do_update(
                    index_elements=["category_name"],
                    set_={
                        "last_updated": stmt.excluded.last_updated,
                    },
                )
                await self.db.execute(stmt)

            await self.db.commit()

            # Fetch all categories from database to return
            result = await self.db.execute(select(DisruptionCategory))
            categories = list(result.scalars().all())

            # Cache the results
            await self.cache.set(cache_key, categories, ttl=ttl)

            logger.info("disruption_categories_fetched_and_cached", count=len(categories), ttl=ttl)
            return categories

        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_disruption_categories_failed", error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to fetch disruption categories from TfL API.",
            ) from e

    async def fetch_stop_types(self, use_cache: bool = True) -> list[StopType]:
        """
        Fetch stop types metadata from TfL API.

        Args:
            use_cache: Whether to use Redis cache (default: True)

        Returns:
            List of StopType objects from database (filtered to relevant types)
        """
        cache_key = "stop_types:all"

        # Try cache first
        if use_cache:
            cached_types: list[StopType] | None = await self.cache.get(cache_key)
            if cached_types is not None:
                logger.debug("stop_types_cache_hit", count=len(cached_types))
                return cached_types

        logger.info("fetching_stop_types_from_tfl_api")

        try:
            # Fetch from TfL API
            with tfl_api_span("MetaStopTypes", "stoppoint_client") as span:
                response = await self.stoppoint_client.MetaStopTypes()
                if hasattr(response, "http_status_code"):
                    span.set_attribute("http.status_code", response.http_status_code)

            # Check for API error
            self._handle_api_error(response)
            assert not isinstance(response, ApiError)  # Type narrowing for mypy

            # Extract cache TTL from response
            ttl = self._extract_cache_ttl(response) or DEFAULT_METADATA_CACHE_TTL

            # Process and upsert stop types (avoids race conditions)
            # response.content is a RootModel array, access via .root
            type_data_list = response.content.root

            # Filter to relevant types for our use case
            # These types cover tube, rail, and bus stations which are the main transport modes
            # we're interested in for this application. To extend to other types (e.g., tram, ferry),
            # add the appropriate Naptan type to this set.
            relevant_types = {"NaptanMetroStation", "NaptanRailStation", "NaptanBusCoachStation"}

            now = datetime.now(UTC)
            for type_data in type_data_list:
                # type_data might be a string or object - handle both cases
                type_name = type_data if isinstance(type_data, str) else getattr(type_data, "stopType", str(type_data))

                # Only store relevant types
                if type_name in relevant_types:
                    # Use PostgreSQL INSERT ... ON CONFLICT to atomically upsert
                    stmt = insert(StopType).values(
                        type_name=type_name,
                        description=None,  # API typically only provides type name
                        last_updated=now,
                    )
                    stmt = stmt.on_conflict_do_update(
                        index_elements=["type_name"],
                        set_={
                            "last_updated": stmt.excluded.last_updated,
                        },
                    )
                    await self.db.execute(stmt)

            await self.db.commit()

            # Fetch all relevant types from database to return
            result = await self.db.execute(select(StopType).where(StopType.type_name.in_(relevant_types)))
            types = list(result.scalars().all())

            # Cache the results
            await self.cache.set(cache_key, types, ttl=ttl)

            logger.info("stop_types_fetched_and_cached", count=len(types), ttl=ttl)
            return types

        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_stop_types_failed", error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to fetch stop types from TfL API.",
            ) from e

    async def refresh_metadata_with_change_detection(
        self,
    ) -> tuple[int, int, int]:
        """
        Refresh all TfL metadata and detect changes.

        Fetches fresh metadata from TfL API and compares with current database state.
        Raises MetadataChangeDetectedError if any changes are detected.

        This method is used by scheduled Celery tasks to ensure metadata stays
        current while alerting on unexpected changes (which are rare for "super
        static" data like severity codes and disruption categories).

        Returns:
            Tuple of (severity_codes_count, disruption_categories_count, stop_types_count)

        Raises:
            MetadataChangeDetectedError: If metadata has changed
            HTTPException: If TfL API calls fail

        Example:
            >>> counts = await tfl_service.refresh_metadata_with_change_detection()
            >>> print(f"Refreshed {counts[0]} severity codes")
        """
        try:
            # Step 1: Fetch current state from database
            severity_codes_before = list((await self.db.execute(select(SeverityCode))).scalars().all())
            disruption_categories_before = list((await self.db.execute(select(DisruptionCategory))).scalars().all())
            stop_types_before = list((await self.db.execute(select(StopType))).scalars().all())

            # Step 2: Compute hashes of current state
            severity_hash_before = _compute_metadata_hash(severity_codes_before)
            categories_hash_before = _compute_metadata_hash(disruption_categories_before)
            types_hash_before = _compute_metadata_hash(stop_types_before)

            logger.info(
                "metadata_refresh_started",
                severity_codes_count=len(severity_codes_before),
                disruption_categories_count=len(disruption_categories_before),
                stop_types_count=len(stop_types_before),
                severity_hash=severity_hash_before[:8],  # First 8 chars for logging
                categories_hash=categories_hash_before[:8],
                types_hash=types_hash_before[:8],
            )

            # Step 3: Fetch fresh data from TfL API (bypass cache)
            severity_codes_after = await self.fetch_severity_codes(use_cache=False)
            disruption_categories_after = await self.fetch_disruption_categories(use_cache=False)
            stop_types_after = await self.fetch_stop_types(use_cache=False)

            # Step 4: Compute hashes of new state
            severity_hash_after = _compute_metadata_hash(severity_codes_after)
            categories_hash_after = _compute_metadata_hash(disruption_categories_after)
            types_hash_after = _compute_metadata_hash(stop_types_after)

            # Check if this is initial population (empty database)
            is_initial_population = (
                not severity_codes_before and not disruption_categories_before and not stop_types_before
            )

            if is_initial_population:
                logger.info(
                    "initial_metadata_population",
                    severity_codes_count=len(severity_codes_after),
                    disruption_categories_count=len(disruption_categories_after),
                    stop_types_count=len(stop_types_after),
                )
                return (
                    len(severity_codes_after),
                    len(disruption_categories_after),
                    len(stop_types_after),
                )

            # Step 5: Detect changes
            changes_detected = []
            if severity_hash_before != severity_hash_after:
                changes_detected.append("severity_codes")
            if categories_hash_before != categories_hash_after:
                changes_detected.append("disruption_categories")
            if types_hash_before != types_hash_after:
                changes_detected.append("stop_types")

            # Step 6: Raise exception if changes detected
            if changes_detected:
                details = {
                    "changed_types": changes_detected,
                    "severity_codes": {
                        "before_count": len(severity_codes_before),
                        "after_count": len(severity_codes_after),
                        "before_hash": severity_hash_before,
                        "after_hash": severity_hash_after,
                    },
                    "disruption_categories": {
                        "before_count": len(disruption_categories_before),
                        "after_count": len(disruption_categories_after),
                        "before_hash": categories_hash_before,
                        "after_hash": categories_hash_after,
                    },
                    "stop_types": {
                        "before_count": len(stop_types_before),
                        "after_count": len(stop_types_after),
                        "before_hash": types_hash_before,
                        "after_hash": types_hash_after,
                    },
                }
                logger.error(
                    "metadata_changed_unexpectedly",
                    changes=changes_detected,
                    details=details,
                )
                msg = f"TfL metadata changed unexpectedly: {', '.join(changes_detected)}"
                raise MetadataChangeDetectedError(
                    msg,
                    details=details,
                    before_counts=(
                        len(severity_codes_before),
                        len(disruption_categories_before),
                        len(stop_types_before),
                    ),
                    after_counts=(
                        len(severity_codes_after),
                        len(disruption_categories_after),
                        len(stop_types_after),
                    ),
                )

            logger.info(
                "metadata_refresh_completed_no_changes",
                severity_codes_count=len(severity_codes_after),
                disruption_categories_count=len(disruption_categories_after),
                stop_types_count=len(stop_types_after),
            )

            return (
                len(severity_codes_after),
                len(disruption_categories_after),
                len(stop_types_after),
            )

        except MetadataChangeDetectedError:
            # Re-raise to preserve exception details
            raise
        except HTTPException:
            # Re-raise HTTP exceptions from fetch methods
            raise
        except Exception as e:
            logger.error("metadata_refresh_failed", error=str(e), error_type=type(e).__name__, exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to refresh TfL metadata.",
            ) from e

    async def _extract_hub_fields(self, stop_point: StopPoint) -> tuple[str | None, str | None]:
        """
        Extract hub NaPTAN code and fetch hub common name from TfL API.

        Hub information is used to identify cross-mode interchange stations.
        If a hub NaPTAN code exists, makes an API call to fetch the hub details
        and extracts the common name (e.g., "Seven Sisters" for hub "HUBSVS").

        The API call is executed in a thread pool to avoid blocking the event loop.
        The pydantic-tfl-api client is thread-safe as it uses httpx for HTTP requests.

        Args:
            stop_point: Stop point object from TfL API

        Returns:
            Tuple of (hub_naptan_code, hub_common_name)
        """
        hub_code = getattr(stop_point, "hubNaptanCode", None)
        hub_name = None

        # Fetch hub details from API if hub code exists
        if hub_code:
            try:
                with tfl_api_span(
                    "GetByPathIdsQueryIncludeCrowdingData",
                    "stoppoint_client",
                    **{"tfl.api.hub_code": hub_code},
                ) as span:
                    response = await self.stoppoint_client.GetByPathIdsQueryIncludeCrowdingData(
                        hub_code,
                        False,  # includeCrowdingData
                    )
                    if hasattr(response, "http_status_code"):
                        span.set_attribute("http.status_code", response.http_status_code)
                if not isinstance(response, ApiError) and response.content and response.content.root:
                    hub_data = response.content.root[0]
                    hub_name = getattr(hub_data, "commonName", None)
            except Exception as e:
                # Log error but don't fail - hub name is optional
                logger.warning(
                    "failed_to_fetch_hub_details",
                    hub_code=hub_code,
                    error=str(e),
                )

        return hub_code, hub_name

    def _update_existing_station(
        self,
        station: Station,
        line_tfl_id: str,
        hub_code: str | None,
        hub_name: str | None,
    ) -> None:
        """
        Update existing station with line and hub information.

        Args:
            station: Existing station object to update
            line_tfl_id: TfL line ID to add to station's lines
            hub_code: Hub NaPTAN code (or None)
            hub_name: Hub common name (or None)
        """
        if line_tfl_id not in station.lines:
            station.lines = [*station.lines, line_tfl_id]
        station.last_updated = datetime.now(UTC)
        station.hub_naptan_code = hub_code
        station.hub_common_name = hub_name

    def _create_new_station(
        self,
        stop_point: StopPoint,
        line_tfl_id: str,
        hub_code: str | None,
        hub_name: str | None,
    ) -> Station:
        """
        Create new station from stop point data.

        Args:
            stop_point: Stop point object from TfL API
            line_tfl_id: TfL line ID for the station
            hub_code: Hub NaPTAN code (or None)
            hub_name: Hub common name (or None)

        Returns:
            New Station object (not yet added to session)
        """
        return Station(
            tfl_id=stop_point.id,
            name=stop_point.commonName,
            latitude=stop_point.lat,
            longitude=stop_point.lon,
            lines=[line_tfl_id],
            last_updated=datetime.now(UTC),
            hub_naptan_code=hub_code,
            hub_common_name=hub_name,
        )

    async def _fetch_stations_from_api(self, line_tfl_id: str) -> tuple[list[Station], int]:
        """
        Fetch stations for a line from TfL API and update database.

        This fetches ALL stations on a line, including non-TfL-operated National Rail stations.

        Args:
            line_tfl_id: TfL line ID to fetch stations for

        Returns:
            Tuple of (list of Station objects, cache TTL in seconds)
        """
        # Fetch stations for the line using LineClient with tflOperatedNationalRailStationsOnly=False
        # This ensures we get ALL stations, not just TfL-operated ones
        with tfl_api_span(
            "StopPointsByPathIdQueryTflOperatedNationalRailStationsOnly",
            "line_client",
            **{"tfl.api.line_id": line_tfl_id},
        ) as span:
            response = await self.line_client.StopPointsByPathIdQueryTflOperatedNationalRailStationsOnly(
                line_tfl_id,
                False,  # tflOperatedNationalRailStationsOnly=False to get ALL stations
            )
            if hasattr(response, "http_status_code"):
                span.set_attribute("http.status_code", response.http_status_code)

        # Check for API error
        self._handle_api_error(response)
        assert not isinstance(response, ApiError)  # Type narrowing for mypy

        ttl = self._extract_cache_ttl(response) or DEFAULT_STATIONS_CACHE_TTL
        # response.content is a StopPointArray (RootModel), access via .root
        stop_points = response.content.root

        stations = []
        # Note: This implementation queries the database inside the loop (N+1 pattern).
        # For this hobby project, this is acceptable because:
        # - TfL lines typically have 20-60 stations (small N)
        # - This operation is infrequent (cached station data, runs ~daily)
        # - Database queries are fast (local DB, indexed tfl_id)
        # Bulk fetching could be considered if performance monitoring indicates a problem.
        for stop_point in stop_points:
            # Filter stations by mode - only keep stations with at least one mode in DEFAULT_MODES
            modes = getattr(stop_point, "modes", []) or []  # Handle None case
            if not any(mode in DEFAULT_MODES for mode in modes):
                logger.debug(
                    "station_filtered_by_mode",
                    station_id=stop_point.id,
                    station_name=stop_point.commonName,
                    modes=modes,
                    reason="no_overlap_with_default_modes",
                )
                continue

            # Check if station exists in DB
            result = await self.db.execute(select(Station).where(Station.tfl_id == stop_point.id))
            station = result.scalar_one_or_none()

            # Extract hub fields once
            hub_code, hub_name = await self._extract_hub_fields(stop_point)

            # Log hub detection
            if hub_code:
                logger.debug(
                    "hub_detected",
                    station_id=stop_point.id,
                    station_name=stop_point.commonName,
                    hub_code=hub_code,
                    hub_name=hub_name,
                )

            if station:
                # Update existing station
                self._update_existing_station(station, line_tfl_id, hub_code, hub_name)
            else:
                # Create new station
                station = self._create_new_station(stop_point, line_tfl_id, hub_code, hub_name)
                self.db.add(station)

            stations.append(station)

        await self.db.commit()

        # Refresh to get database IDs
        for station in stations:
            await self.db.refresh(station)

        return stations, ttl

    async def fetch_stations(
        self, line_tfl_id: str | None = None, use_cache: bool = True, skip_database_validation: bool = False
    ) -> list[Station]:
        """
        Fetch stations from database or TfL API (during graph building).

        For public endpoints, this reads from database only. During graph building
        (skip_database_validation=True), it calls TfL API to populate the database.

        Args:
            line_tfl_id: Optional TfL line ID to filter stations (e.g., "victoria")
            use_cache: Whether to use Redis cache (default: True)
            skip_database_validation: If True, skip validation and call TfL API directly.
                Used during graph building to populate database. Default: False.

        Returns:
            List of Station objects

        Raises:
            HTTPException(503): If database not initialized (and skip_database_validation=False)
            HTTPException(404): If line doesn't exist (and skip_database_validation=False)
            HTTPException(404): If line exists but no stations found (and skip_database_validation=False)
        """
        cache_key = build_station_cache_key(line_tfl_id)

        # Try cache first
        if use_cache:
            cached_stations: list[Station] | None = await self.cache.get(cache_key)
            if cached_stations is not None:
                logger.debug("stations_cache_hit", line_tfl_id=line_tfl_id, count=len(cached_stations))
                return cached_stations

        # Route to appropriate fetch method
        if skip_database_validation:
            # Graph building always requires a specific line_tfl_id
            if line_tfl_id is None:
                msg = "line_tfl_id is required when skip_database_validation=True"
                raise ValueError(msg)
            return await self._fetch_stations_for_graph_building(line_tfl_id, cache_key)
        return await self._fetch_stations_from_database(line_tfl_id, cache_key)

    async def _fetch_stations_for_graph_building(self, line_tfl_id: str, cache_key: str) -> list[Station]:
        """Fetch stations from TfL API for graph building (skips validation)."""
        logger.info("fetching_stations_from_tfl_api", line_tfl_id=line_tfl_id)
        try:
            stations, ttl = await self._fetch_stations_from_api(line_tfl_id)

            # Cache the results
            await self.cache.set(cache_key, stations, ttl=ttl)
            logger.info("stations_fetched_and_cached", line_tfl_id=line_tfl_id, count=len(stations), ttl=ttl)
            return stations
        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_stations_failed", line_tfl_id=line_tfl_id, error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to fetch stations from TfL API.",
            ) from e

    async def _fetch_stations_from_database(self, line_tfl_id: str | None, cache_key: str) -> list[Station]:
        """Fetch stations from database with validation (public endpoint, prevents API quota exhaustion)."""
        logger.info("fetching_stations_from_database", line_tfl_id=line_tfl_id)

        try:
            # Check if database has been initialized
            station_count_result = await self.db.execute(select(func.count(Station.id)))
            station_count = station_count_result.scalar() or 0

            if not is_database_initialized(station_count):
                logger.error("tfl_data_not_initialized")
                raise DatabaseNotInitializedError

            if line_tfl_id:
                # Validate line exists
                line_result = await self.db.execute(select(Line).where(Line.tfl_id == line_tfl_id))
                line = line_result.scalar_one_or_none()

                if line is None:
                    logger.warning("line_not_found", line_tfl_id=line_tfl_id)
                    raise LineNotFoundError(line_tfl_id)

                # Fetch all stations and filter by line using helper
                result = await self.db.execute(select(Station))
                all_stations = list(result.scalars().all())
                stations = filter_stations_by_line_tfl_id(all_stations, line_tfl_id)

                # Validate at least one station found
                validate_stations_exist_for_line(stations, line_tfl_id)
            else:
                # Fetch all stations from database
                result = await self.db.execute(select(Station))
                stations = list(result.scalars().all())

            # Cache the results
            ttl = DEFAULT_STATIONS_CACHE_TTL
            await self.cache.set(cache_key, stations, ttl=ttl)
            logger.info("stations_fetched_and_cached", line_tfl_id=line_tfl_id, count=len(stations), ttl=ttl)
            return stations

        except DatabaseNotInitializedError as e:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=str(e),
            ) from e
        except (LineNotFoundError, NoStationsForLineError) as e:
            # Log at appropriate level based on error type
            if isinstance(e, NoStationsForLineError):
                logger.warning("no_stations_for_line", line_tfl_id=line_tfl_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=str(e),
            ) from e
        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_stations_failed", line_tfl_id=line_tfl_id, error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Failed to fetch stations from database.",
            ) from e

    def deduplicate_stations_by_hub(self, stations: list[Station], line_filter: str | None = None) -> list[Station]:
        """
        Deduplicate stations by grouping hub children into single representative stations.

        This method takes a list of stations and groups those that share a hub_naptan_code
        into single representative stations. Each hub representative uses the hub code as
        its tfl_id, aggregates lines from all child stations, and uses the hub_common_name.

        When a line_filter is provided, the hub representative's UUID and other properties
        are taken from the child station that serves that line (if available), making the
        response contextually relevant to the query.

        Standalone stations (without hub_naptan_code) are returned unchanged.

        This method orchestrates pure helper functions from app.helpers.station_resolution
        for testable, functional-style deduplication logic.

        Args:
            stations: List of Station objects to deduplicate
            line_filter: Optional line ID to prefer when selecting hub representative template.
                        If provided, uses the child station serving this line for UUID/coordinates.

        Returns:
            List of deduplicated stations with hubs represented as single entries,
            sorted alphabetically by name

        Examples:
            >>> # Seven Sisters has both Rail and Tube stations
            >>> rail = Station(tfl_id="910GSEVNSIS", hub_naptan_code="HUBSVS", lines=["overground"])
            >>> tube = Station(tfl_id="940GZZLUSVS", hub_naptan_code="HUBSVS", lines=["victoria"])
            >>> standalone = Station(tfl_id="940GZZLUOXC", hub_naptan_code=None, lines=["piccadilly"])
            >>> service.deduplicate_stations_by_hub([rail, tube, standalone])
            [
                Station(tfl_id="HUBSVS", name="Seven Sisters", lines=["overground", "victoria"]),
                Station(tfl_id="940GZZLUOXC", name="Oxford Circus", lines=["piccadilly"])
            ]

        See Also:
            - group_stations_by_hub: Groups stations into hub groups and standalone lists
            - create_hub_representative: Creates representative station from hub children
        """
        # Group stations by hub using pure helper
        hub_groups, standalone_stations = group_stations_by_hub(stations)

        # Create hub representatives
        result: list[Station] = []
        for hub_children in hub_groups.values():
            # If line filter provided, prefer child serving that line for UUID/coords
            preferred_child = None
            # Find child stations that serve the filtered line and sort for deterministic selection
            if line_filter and (matching := [child for child in hub_children if line_filter in child.lines]):
                preferred_child = sorted(matching, key=lambda s: s.tfl_id)[0]

            hub_representative = create_hub_representative(hub_children, preferred_child)
            result.append(hub_representative)

        # Add standalone stations
        result.extend(standalone_stations)

        # Sort alphabetically by name for consistent ordering
        return sorted(result, key=lambda s: s.name)

    # ==================== Pure Functional Helpers for Disruption Extraction ====================

    @staticmethod
    def _parse_tfl_timestamp(timestamp_str: str | None) -> datetime | None:
        """
        Parse TfL API timestamp string into datetime object.

        Logs warnings for invalid/missing timestamps. Not purely functional due to logging side effects.

        Args:
            timestamp_str: ISO format timestamp string (may include "Z" suffix)

        Returns:
            Parsed datetime object, or None if timestamp is missing/invalid
        """
        if not timestamp_str:
            logger.warning("TfL timestamp is None or empty")
            return None

        with contextlib.suppress(ValueError, AttributeError):
            return datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))

        logger.warning("Failed to parse TfL timestamp", timestamp=timestamp_str)
        return None

    @staticmethod
    def _extract_naptan_id_from_stop_point(stop_point: StopPoint | None) -> str | None:
        """
        Extract NaPTAN ID from a stop point object.

        Pure function: No side effects, returns None if extraction fails.

        Args:
            stop_point: TfL API StopPoint object

        Returns:
            NaPTAN ID string, or None if not found
        """
        return None if not stop_point else getattr(stop_point, "naptanId", None)

    @staticmethod
    def _extract_naptan_codes_from_sequence(
        sequence: list[RouteSectionNaptanEntrySequence] | None,
    ) -> list[str]:
        """
        Extract all valid NaPTAN codes from a route section sequence.

        Pure function: No side effects, filters out invalid/missing codes.

        Args:
            sequence: List of route section entries containing stopPoint objects

        Returns:
            List of NaPTAN code strings (empty if sequence is None or contains no valid codes)
        """
        if not sequence:
            return []

        naptan_codes = []
        for seq_item in sequence:
            stop_point = getattr(seq_item, "stopPoint", None)
            if naptan_id := TfLService._extract_naptan_id_from_stop_point(stop_point):
                naptan_codes.append(naptan_id)

        return naptan_codes

    @staticmethod
    def _build_affected_route_info(
        route_name: str, route_direction: str, affected_stations: list[str]
    ) -> AffectedRouteInfo | None:
        """
        Build AffectedRouteInfo object if affected stations exist.

        Pure function: No side effects, returns None if no affected stations.

        Args:
            route_name: Name of the affected route
            route_direction: Direction of travel (inbound/outbound)
            affected_stations: List of NaPTAN codes for affected stations

        Returns:
            AffectedRouteInfo object, or None if affected_stations is empty
        """
        if not affected_stations:
            return None

        return AffectedRouteInfo(
            name=route_name,
            direction=route_direction,
            affected_stations=affected_stations,
        )

    @staticmethod
    def _process_affected_route(route: RouteSection) -> AffectedRouteInfo | None:
        """
        Process a single affected route from TfL API into AffectedRouteInfo.

        Pure function: No side effects, returns None if required fields are missing.

        Args:
            route: TfL API RouteSection object from disruption.affectedRoutes

        Returns:
            AffectedRouteInfo object, or None if route is invalid or incomplete
        """
        route_name: str | None = getattr(route, "name", None)
        route_direction: str | None = getattr(route, "direction", None)
        route_section_sequence: list[RouteSectionNaptanEntrySequence] | None = getattr(
            route, "routeSectionNaptanEntrySequence", None
        )

        # Require all fields to be present
        if not (route_name and route_direction and route_section_sequence):
            return None

        # Extract station codes
        affected_stations = TfLService._extract_naptan_codes_from_sequence(route_section_sequence)

        # Build route info (returns None if no valid stations)
        return TfLService._build_affected_route_info(route_name, route_direction, affected_stations)

    @staticmethod
    def _extract_affected_routes_from_disruption(disruption: Disruption | None) -> list[AffectedRouteInfo] | None:
        """
        Extract all valid affected routes from a disruption object.

        Pure function: No side effects, returns None if no valid routes found.

        Args:
            disruption: TfL API Disruption object containing affectedRoutes

        Returns:
            List of AffectedRouteInfo objects, or None if no valid routes exist
        """
        if not disruption:
            return None

        tfl_affected_routes = getattr(disruption, "affectedRoutes", None)
        if not tfl_affected_routes:
            return None

        # Process all routes, filtering out None values
        valid_routes = [
            route_info
            for route in tfl_affected_routes
            if (route_info := TfLService._process_affected_route(route)) is not None
        ]

        # Return None if no valid routes were extracted
        return valid_routes or None

    @staticmethod
    def _extract_reason_from_sources(line_status: LineStatus, disruption: Disruption | None) -> str | None:
        """
        Extract reason/description from line status or disruption object.

        Pure function: No side effects, tries line_status first, then disruption.

        Args:
            line_status: TfL API LineStatus object
            disruption: TfL API Disruption object (may be None)

        Returns:
            Reason string, or None if not found in either source
        """
        if (reason := getattr(line_status, "reason", None)) is not None:
            return str(reason)

        if disruption:
            description: str | None = getattr(disruption, "description", None)
            return description

        return None

    @staticmethod
    def _extract_created_timestamp(line_status: LineStatus, disruption: Disruption | None) -> str | None:
        """
        Extract created timestamp from line status or disruption object.

        Pure function: No side effects, tries line_status first, then disruption.

        Args:
            line_status: TfL API LineStatus object
            disruption: TfL API Disruption object (may be None)

        Returns:
            Timestamp string, or None if not found in either source
        """
        if (created_str := getattr(line_status, "created", None)) is not None:
            return str(created_str)

        if disruption:
            created_from_disruption: str | None = getattr(disruption, "created", None)
            return created_from_disruption

        return None

    # ==================== Main Disruption Extraction ====================

    def _extract_disruption_from_line_status(
        self,
        line_status: LineStatus,
        line_id: str,
        line_name: str,
        mode: str,
    ) -> DisruptionResponse:
        """
        Extract line status information from a LineStatus object.

        Returns status for all severity levels (including Good Service) - filtering
        is left to the caller or frontend.

        Args:
            line_status: LineStatus object from TfL API containing statusSeverity and nested disruption
            line_id: ID of the line
            line_name: Name of the line
            mode: Transport mode (tube, dlr, overground, etc.)

        Returns:
            DisruptionResponse object containing current line status
        """
        # Extract severity (authoritative source)
        status_severity = getattr(line_status, "statusSeverity", 0)
        status_severity_description = getattr(line_status, "statusSeverityDescription", "Unknown")

        # Get optional disruption details
        disruption = getattr(line_status, "disruption", None)

        # Extract reason and timestamp using pure helpers
        reason = self._extract_reason_from_sources(line_status, disruption)
        created_str = self._extract_created_timestamp(line_status, disruption)
        created_at = self._parse_tfl_timestamp(created_str)

        # Extract affected routes using pure helper
        affected_routes = self._extract_affected_routes_from_disruption(disruption)

        return DisruptionResponse(
            line_id=line_id,
            line_name=line_name,
            mode=mode,
            status_severity=status_severity,
            status_severity_description=status_severity_description,
            reason=reason,
            created_at=created_at,
            affected_routes=affected_routes,
        )

    def _process_line_status_data(
        self,
        line_data: TflLine,
    ) -> list[DisruptionResponse]:
        """
        Process Line object containing LineStatus data into structured line status responses.

        Each Line can have multiple LineStatus objects representing different statuses
        (e.g., PlannedWork, RealTime issues, Good Service). All returned statuses are
        already filtered by TfL API to be currently active (because we query without
        StartDate/EndDate parameters).

        Args:
            line_data: Line object from TfL API containing lineStatuses

        Returns:
            List of line status responses for all currently active statuses (including Good Service and disruptions)
        """
        disruptions: list[DisruptionResponse] = []
        line_id = getattr(line_data, "id", None) or "unknown"
        line_name = getattr(line_data, "name", None) or "Unknown"
        mode_name = getattr(line_data, "modeName", None) or "unknown"
        line_statuses = getattr(line_data, "lineStatuses", None)

        if not line_statuses:
            return disruptions

        for line_status in line_statuses:
            # All statuses from API are already filtered to current time
            # (because we don't pass StartDate/EndDate parameters)
            disruption = self._extract_disruption_from_line_status(
                line_status,
                line_id,
                line_name,
                mode_name,
            )
            disruptions.append(disruption)

        return disruptions

    async def fetch_line_disruptions(
        self,
        modes: list[str] | None = None,
        use_cache: bool = True,
    ) -> list[DisruptionResponse]:
        """
        Fetch current line status information from TfL API for specified modes.

        Uses the StatusByIds endpoint to get line status information with detailed
        severity levels (0-20) directly from TfL API. This includes all line statuses
        (Good Service, disruptions, planned work, etc.). By querying without StartDate/EndDate
        parameters, the API automatically filters to only return currently active statuses.

        Args:
            modes: List of transport modes to fetch statuses for.
                   If None, defaults to ["tube", "overground", "dlr", "elizabeth-line"].
            use_cache: Whether to use Redis cache (default: True)

        Returns:
            List of line status responses (including both disruptions and good service)
            for currently active statuses
        """
        # Default to major transport modes if not specified
        if modes is None:
            modes = DEFAULT_MODES

        cache_key = self._build_modes_cache_key("line_disruptions", modes)

        # Try cache first
        if use_cache:
            cached_disruptions: list[DisruptionResponse] | None = await self.cache.get(cache_key)
            if cached_disruptions is not None:
                logger.debug("line_disruptions_cache_hit", count=len(cached_disruptions), modes=modes)
                return cached_disruptions

        logger.info("fetching_line_disruptions_from_tfl_api", modes=modes)

        try:
            all_disruptions = []

            # First, fetch all lines for the specified modes to get line IDs
            lines = await self.fetch_lines(modes=modes, use_cache=use_cache)
            line_ids = [line.tfl_id for line in lines if line.tfl_id]

            if not line_ids:
                logger.warning("no_lines_found_for_modes", modes=modes)
                return []

            # Join line IDs with commas for the StatusByIds endpoint
            line_ids_str = ",".join(line_ids)
            logger.debug("fetching_status_for_lines", line_count=len(line_ids), modes=modes)

            # Fetch line status for all lines at once
            with tfl_api_span(
                "StatusByIdsByPathIdsQueryDetail",
                "line_client",
                **{"tfl.api.line_count": len(line_ids)},
            ) as span:
                response = await self.line_client.StatusByIdsByPathIdsQueryDetail(
                    line_ids_str,
                    True,  # detail=True to get disruption details
                )
                if hasattr(response, "http_status_code"):
                    span.set_attribute("http.status_code", response.http_status_code)

            # Check for API error
            self._handle_api_error(response)
            assert not isinstance(response, ApiError)  # Type narrowing for mypy

            # Extract cache TTL from response
            ttl = self._extract_cache_ttl(response) or DEFAULT_DISRUPTIONS_CACHE_TTL

            # Process line status data
            # response.content is a RootModel array of Line objects, access via .root
            line_data_list = response.content.root

            for line_data in line_data_list:
                line_disruptions = self._process_line_status_data(line_data)
                all_disruptions.extend(line_disruptions)

            logger.debug("all_lines_processed", total_disruptions=len(all_disruptions))

            # Cache the results
            await self.cache.set(cache_key, all_disruptions, ttl=ttl)

            logger.info(
                "line_disruptions_fetched_and_cached",
                count=len(all_disruptions),
                modes=modes,
                ttl=ttl,
            )
            return all_disruptions

        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_line_disruptions_failed", error=str(e), modes=modes, exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Failed to fetch line disruptions from TfL API for modes: {modes}",
            ) from e

    async def _create_station_disruption_from_point(
        self,
        station: Station,
        disrupted_point: DisruptedPoint,
    ) -> StationDisruptionResponse:
        """Create StationDisruption from TfL API DisruptedPoint and persist to database.

        Maps DisruptedPoint fields to our StationDisruption model:
        - type  type (e.g., 'Information', 'Interchange Message')
        - appearance  appearance (e.g., 'PlannedWork', 'RealTime')
        - description  description
        - fromDate  created_at_source
        - toDate  end_date
        - (generated hash)  tfl_id

        Args:
            station: Station model from database (already looked up)
            disrupted_point: DisruptedPoint object from TfL API

        Returns:
            StationDisruptionResponse with data persisted to database

        Raises:
            None - uses safe defaults for missing optional fields
        """
        # Extract fields from DisruptedPoint (using type-safe getattr for optional fields)
        disruption_type: str | None = getattr(disrupted_point, "type", None)
        appearance: str | None = getattr(disrupted_point, "appearance", None)
        # Handle None or missing description (field must not be NULL in DB)
        description: str = getattr(disrupted_point, "description", None) or "No description available"

        # Parse timestamps using pure helper function
        from_date_str: str | None = getattr(disrupted_point, "fromDate", None)
        to_date_str: str | None = getattr(disrupted_point, "toDate", None)

        created_at_source = _parse_tfl_timestamp(from_date_str)
        if created_at_source is None:
            logger.warning(
                "disruption_missing_from_date",
                from_date_str=from_date_str,
                disrupted_point=str(disrupted_point)[:200],
            )
            created_at_source = datetime.now(UTC)
        end_date = _parse_tfl_timestamp(to_date_str)

        # Generate TfL ID using pure helper function
        # Use station.tfl_id (ATCO code) from database - already validated by caller
        station_atco = _extract_station_atco_code(disrupted_point)
        if not station_atco:
            logger.warning(
                "disruption_missing_atco_code",
                station_tfl_id=station.tfl_id,
                disrupted_point=str(disrupted_point)[:200],
            )
            # Use station.tfl_id as fallback (already validated by caller)
            station_atco = station.tfl_id
        tfl_id = _generate_station_disruption_tfl_id(
            station_atco,
            created_at_source,
            end_date,
            description,
        )

        # Create database record
        station_disruption = StationDisruption(
            station_id=station.id,
            type=disruption_type,
            description=description,
            appearance=appearance,
            tfl_id=tfl_id,
            created_at_source=created_at_source,
            end_date=end_date,
        )

        self.db.add(station_disruption)
        await self.db.flush()
        await self.db.refresh(station_disruption)

        # Build response object
        return StationDisruptionResponse(
            station_id=station.id,
            station_tfl_id=station.tfl_id,
            station_name=station.name,
            type=disruption_type,
            description=description,
            appearance=appearance,
            tfl_id=tfl_id,
            created_at_source=created_at_source,
            end_date=end_date,
        )

    async def _lookup_station_for_disruption(self, stop: StopPoint | MatchedStop) -> Station | None:
        """
        Look up station in database for a disruption stop.

        Args:
            stop: Stop point data object from TfL API

        Returns:
            Station object or None if not found or invalid
        """
        stop_tfl_id = self._get_stop_ids(stop)

        if not stop_tfl_id:
            logger.warning("station_disruption_missing_tfl_id", stop_data=str(stop))
            return None

        result = await self.db.execute(select(Station).where(Station.tfl_id == stop_tfl_id))
        station = result.scalar_one_or_none()

        if not station:
            logger.debug(
                "station_not_found_for_disruption",
                stop_tfl_id=stop_tfl_id,
            )
            return None

        return station

    async def _process_station_disruption_data(
        self,
        disruption_data_list: list[DisruptedPoint],
    ) -> list[StationDisruptionResponse]:
        """Process TfL API DisruptedPoint data into StationDisruption records.

        Each DisruptedPoint represents a single station-level disruption (e.g., lift outage,
        entrance closure, accessibility issue) as opposed to line-level disruptions.

        Args:
            disruption_data_list: List of DisruptedPoint objects from TfL API
                                  (from StopPoint.DisruptionByMode endpoint)

        Returns:
            List of StationDisruptionResponse objects (persisted to database)

        Note:
            Stations not found in database are skipped with debug log (users can't route
            through stations not in the network graph anyway).
        """
        mode_disruptions: list[StationDisruptionResponse] = []

        for disrupted_point in disruption_data_list:
            # Extract station ATCO code using pure helper function
            station_code = _extract_station_atco_code(disrupted_point)
            if not station_code:
                logger.warning("disrupted_point_missing_station_code", point=disrupted_point)
                continue

            # Look up station in database
            result = await self.db.execute(select(Station).where(Station.tfl_id == station_code))
            station = result.scalar_one_or_none()

            if not station:
                logger.debug("station_not_found_for_disruption", atco_code=station_code)
                continue

            # Create disruption record from DisruptedPoint
            disruption_response = await self._create_station_disruption_from_point(station, disrupted_point)
            mode_disruptions.append(disruption_response)

        return mode_disruptions

    async def fetch_station_disruptions(
        self,
        modes: list[str] | None = None,
        use_cache: bool = True,
    ) -> list[StationDisruptionResponse]:
        """
        Fetch current station-level disruptions from TfL API for specified modes.

        Uses the StopPoint DisruptionByMode endpoint to get disruptions affecting
        specific stations, including route blocked stops.

        Args:
            modes: List of transport modes to fetch disruptions for.
                   If None, defaults to ["tube", "overground", "dlr", "elizabeth-line"].
            use_cache: Whether to use Redis cache (default: True)

        Returns:
            List of station disruption responses
        """
        # Default to major transport modes if not specified
        if modes is None:
            modes = DEFAULT_MODES

        cache_key = self._build_modes_cache_key("station_disruptions", modes)

        # Try cache first
        if use_cache:
            cached_disruptions: list[StationDisruptionResponse] | None = await self.cache.get(cache_key)
            if cached_disruptions is not None:
                logger.debug("station_disruptions_cache_hit", count=len(cached_disruptions), modes=modes)
                return cached_disruptions

        logger.info("fetching_station_disruptions_from_tfl_api", modes=modes)

        try:
            # Clear existing station disruptions within the same transaction to minimize the gap
            await self.db.execute(delete(StationDisruption))

            all_disruptions: list[StationDisruptionResponse] = []
            ttl = DEFAULT_DISRUPTIONS_CACHE_TTL

            # Fetch station disruptions for each mode
            for mode in modes:
                logger.debug("fetching_station_disruptions_for_mode", mode=mode)

                with tfl_api_span(
                    "DisruptionByModeByPathModesQueryIncludeRouteBlockedStops",
                    "stoppoint_client",
                    **{"tfl.api.mode": mode},
                ) as span:
                    response = await self.stoppoint_client.DisruptionByModeByPathModesQueryIncludeRouteBlockedStops(
                        mode,
                        True,  # includeRouteBlockedStops=True to get all station-level issues
                    )
                    if hasattr(response, "http_status_code"):
                        span.set_attribute("http.status_code", response.http_status_code)

                # Check for API error
                self._handle_api_error(response)
                assert not isinstance(response, ApiError)  # Type narrowing for mypy

                # Extract cache TTL from response (use minimum TTL across all modes)
                mode_ttl = self._extract_cache_ttl(response) or DEFAULT_DISRUPTIONS_CACHE_TTL
                ttl = min(ttl, mode_ttl)

                # Process station disruptions using helper method
                # response.content is a RootModel array of DisruptedPoint objects
                # pydantic-tfl-api types are correct - API returns list[DisruptedPoint]
                disruption_data_list = response.content.root
                mode_disruptions = await self._process_station_disruption_data(disruption_data_list)
                all_disruptions.extend(mode_disruptions)

                logger.debug("mode_station_disruptions_processed", mode=mode)

            # Commit database changes
            await self.db.commit()

            # Cache the results
            await self.cache.set(cache_key, all_disruptions, ttl=ttl)

            logger.info(
                "station_disruptions_fetched_and_cached",
                count=len(all_disruptions),
                modes=modes,
                ttl=ttl,
            )
            return all_disruptions

        except HTTPException:
            raise
        except Exception as e:
            logger.error("fetch_station_disruptions_failed", error=str(e), modes=modes, exc_info=e)
            await self.db.rollback()
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail=f"Failed to fetch station disruptions from TfL API for modes: {modes}",
            ) from e

    def _get_stop_ids(self, stop: StopPoint | MatchedStop) -> str | None:
        """
        Extract stop ID from stop point data.

        Args:
            stop: Stop point data object

        Returns:
            Stop ID or None if not found
        """
        if hasattr(stop, "id"):
            return str(stop.id)
        if hasattr(stop, "stationId"):
            return str(stop.stationId)
        logger.warning(
            "stop_id_extraction_failed",
            message="Neither 'id' nor 'stationId' found in stop object",
            stop_data=str(stop),
        )
        return None

    async def _get_station_by_tfl_id(self, tfl_id: str) -> Station | None:
        """
        Look up station in database by TfL ID.

        Args:
            tfl_id: TfL station identifier

        Returns:
            Station object or None if not found
        """
        result = await self.db.execute(select(Station).where(Station.tfl_id == tfl_id))
        return result.scalar_one_or_none()

    async def _connection_exists(
        self,
        from_station_id: uuid.UUID,
        to_station_id: uuid.UUID,
        line_id: uuid.UUID,
    ) -> bool:
        """
        Check if a station connection already exists.

        Args:
            from_station_id: Source station database ID
            to_station_id: Destination station database ID
            line_id: Line database ID

        Returns:
            True if connection exists, False otherwise
        """
        result = await self.db.execute(
            select(StationConnection).where(
                StationConnection.from_station_id == from_station_id,
                StationConnection.to_station_id == to_station_id,
                StationConnection.line_id == line_id,
                StationConnection.deleted_at.is_(None),  # Only check active connections
            )
        )
        return result.scalar_one_or_none() is not None

    def _create_connection(
        self,
        from_station_id: uuid.UUID,
        to_station_id: uuid.UUID,
        line_id: uuid.UUID,
    ) -> StationConnection:
        """
        Create a station connection object.

        Args:
            from_station_id: Source station database ID
            to_station_id: Destination station database ID
            line_id: Line database ID

        Returns:
            StationConnection object (not yet added to session)
        """
        return StationConnection(
            from_station_id=from_station_id,
            to_station_id=to_station_id,
            line_id=line_id,
        )

    async def _process_station_pair(
        self,
        current_stop: MatchedStop,
        next_stop: MatchedStop,
        line: Line,
        stations_set: set[str],
        pending_connections: set[tuple[uuid.UUID, uuid.UUID, uuid.UUID]],
    ) -> int:
        """
        Process a pair of consecutive stations and create bidirectional connections.

        Args:
            current_stop: Current stop point data
            next_stop: Next stop point data
            line: Line object
            stations_set: Set to track unique station IDs
            pending_connections: Set to track pending connections (from_id, to_id, line_id)

        Returns:
            Number of new connections created (0, 1, or 2)
        """
        # Extract stop IDs
        current_stop_id = self._get_stop_ids(current_stop)
        next_stop_id = self._get_stop_ids(next_stop)

        if not current_stop_id or not next_stop_id:
            return 0

        # Look up stations
        from_station = await self._get_station_by_tfl_id(current_stop_id)
        to_station = await self._get_station_by_tfl_id(next_stop_id)

        if not from_station or not to_station:
            logger.debug(
                "station_not_found_for_connection",
                from_stop_id=current_stop_id,
                to_stop_id=next_stop_id,
                line_tfl_id=line.tfl_id,
            )
            return 0

        # Track stations
        stations_set.add(from_station.tfl_id)
        stations_set.add(to_station.tfl_id)

        connections_created = 0

        # Create forward connection if needed (check pending set to avoid duplicates in same transaction)
        forward_key = (from_station.id, to_station.id, line.id)
        if forward_key not in pending_connections:
            connection = self._create_connection(from_station.id, to_station.id, line.id)
            self.db.add(connection)
            pending_connections.add(forward_key)
            connections_created += 1

        # Create reverse connection if needed (check pending set to avoid duplicates in same transaction)
        reverse_key = (to_station.id, from_station.id, line.id)
        if reverse_key not in pending_connections:
            connection = self._create_connection(to_station.id, from_station.id, line.id)
            self.db.add(connection)
            pending_connections.add(reverse_key)
            connections_created += 1

        return connections_created

    async def _fetch_route_sequence(
        self,
        line_tfl_id: str,
        direction: str,
    ) -> RouteSequence:
        """
        Fetch route sequence for a line and direction from TfL API.

        Args:
            line_tfl_id: TfL line identifier
            direction: "inbound" or "outbound"

        Returns:
            RouteSequence object containing stopPointSequences and orderedLineRoutes

        Raises:
            Exception: If API call fails
        """
        with tfl_api_span(
            "RouteSequenceByPathIdPathDirectionQueryServiceTypesQueryExcludeCrowding",
            "line_client",
            **{"tfl.api.line_id": line_tfl_id, "tfl.api.direction": direction},
        ) as span:
            response = await self.line_client.RouteSequenceByPathIdPathDirectionQueryServiceTypesQueryExcludeCrowding(
                line_tfl_id,
                direction,
                "",  # serviceTypes (empty for all)
                True,  # excludeCrowding
            )
            if hasattr(response, "http_status_code"):
                span.set_attribute("http.status_code", response.http_status_code)

        # Check for API error
        self._handle_api_error(response)
        assert not isinstance(response, ApiError)  # Type narrowing for mypy

        # Return full route sequence data (contains both stopPointSequences and orderedLineRoutes)
        return response.content

    async def _process_route_sequence(
        self,
        line: Line,
        direction: str,
        stations_set: set[str],
        pending_connections: set[tuple[uuid.UUID, uuid.UUID, uuid.UUID]],
    ) -> tuple[int, RouteSequence | None]:
        """
        Process route sequence for a line and direction.

        Args:
            line: Line object
            direction: "inbound" or "outbound"
            stations_set: Set to track unique station IDs
            pending_connections: Set to track pending connections (from_id, to_id, line_id)

        Returns:
            Tuple of (connections_count, route_data)
                - connections_count: Number of connections created
                - route_data: Full RouteSequence object (contains orderedLineRoutes)
        """
        try:
            route_data = await self._fetch_route_sequence(line.tfl_id, direction)
            connections_count = 0

            # Extract stopPointSequences for connection building
            if hasattr(route_data, "stopPointSequences") and route_data.stopPointSequences:
                sequences = list(route_data.stopPointSequences)

                for sequence in sequences:
                    if not hasattr(sequence, "stopPoint") or not sequence.stopPoint:
                        continue

                    stop_points = sequence.stopPoint

                    # Process consecutive station pairs
                    for i in range(len(stop_points) - 1):
                        connections_count += await self._process_station_pair(
                            stop_points[i],
                            stop_points[i + 1],
                            line,
                            stations_set,
                            pending_connections,
                        )

            return connections_count, route_data

        except Exception as e:
            logger.warning(
                "failed_to_process_direction",
                line_tfl_id=line.tfl_id,
                direction=direction,
                error=str(e),
            )
            return 0, None

    def _store_line_routes(
        self,
        line: Line,
        inbound_route_data: RouteSequence | None,
        outbound_route_data: RouteSequence | None,
    ) -> None:
        """
        Extract and store route variants from RouteSequence data.

        Filters to only "Regular" service types and stores ordered station lists
        for each route variant in the Line.route_variants JSON field.

        Args:
            line: Line object to update
            inbound_route_data: RouteSequence data for inbound direction (may be None)
            outbound_route_data: RouteSequence data for outbound direction (may be None)
        """
        routes = []

        # Process both directions
        for direction, route_data in [
            ("inbound", inbound_route_data),
            ("outbound", outbound_route_data),
        ]:
            if not route_data or not hasattr(route_data, "orderedLineRoutes"):
                continue

            ordered_routes = route_data.orderedLineRoutes
            if not ordered_routes:
                continue

            # Filter and transform routes
            for ordered_route in ordered_routes:
                # Only store "Regular" service types (defer Night services for MVP)
                service_type = getattr(ordered_route, "serviceType", None)
                if service_type != "Regular":
                    logger.debug(
                        "skipping_non_regular_service",
                        line_tfl_id=line.tfl_id,
                        service_type=service_type,
                        route_name=getattr(ordered_route, "name", "Unknown"),
                    )
                    continue

                # Extract route data
                route_name = getattr(ordered_route, "name", f"Unknown {direction} route")
                naptan_ids = getattr(ordered_route, "naptanIds", [])

                if not naptan_ids:
                    logger.debug(
                        "skipping_route_without_stations",
                        line_tfl_id=line.tfl_id,
                        route_name=route_name,
                    )
                    continue

                # Store route variant
                routes.append(
                    {
                        "name": route_name,
                        "service_type": service_type,
                        "direction": direction,
                        "stations": naptan_ids,
                    }
                )

        # Update line's route_variants field (stored as JSON in database)
        if routes:
            # Extract station lists for deterministic ordering
            # (TfL API may return routes in any order, causing false change detection)
            station_lists = [route["stations"] for route in routes]
            new_route_variants = _normalize_route_variants(station_lists)

            # Detect route_variants changes and log
            if line.route_variants != new_route_variants:
                now = datetime.now(UTC)
                self.db.add(
                    LineChangeLog(
                        tfl_id=line.tfl_id,
                        change_type="updated",
                        changed_fields=["route_variants"],
                        old_values={"route_variants": line.route_variants},
                        new_values={"route_variants": new_route_variants},
                        detected_at=now,
                        trace_id=get_current_trace_id(),
                    )
                )
                logger.info(
                    "line_route_variants_changed",
                    line_tfl_id=line.tfl_id,
                    old_routes_count=len(line.route_variants.get("routes", [])) if line.route_variants else 0,
                    new_routes_count=len(station_lists),
                )

            line.route_variants = new_route_variants
            logger.info(
                "stored_line_routes",
                line_tfl_id=line.tfl_id,
                routes_count=len(routes),
            )
        else:
            logger.warning(
                "no_regular_routes_found",
                line_tfl_id=line.tfl_id,
            )

    async def build_station_graph(self) -> dict[str, int]:
        """
        Build the station connection graph from TfL API data using actual route sequences.

        This fetches the route sequences (inbound and outbound) for all tube lines
        from the TfL API and populates the StationConnection table with bidirectional
        connections based on the actual order of stations on each route.

        Returns:
            Dictionary with build statistics (lines_count, stations_count, connections_count, hubs_count)

        Raises:
            HTTPException: 500 if graph building fails (old connections preserved via rollback)
            HTTPException: 500 if no stations found after fetching (validation failure)
        """
        logger.info("building_station_graph_start")

        try:
            # Fetch all lines
            lines = await self.fetch_lines(use_cache=False)
            logger.info("lines_fetched", lines_count=len(lines))

            # Fetch stations for all lines BEFORE building connections
            # This ensures stations exist in the database for connection creation
            # Use skip_database_validation=True to allow TfL API calls during initial graph building
            for line in lines:
                logger.info("fetching_stations_for_line", line_tfl_id=line.tfl_id, line_name=line.name)
                await self.fetch_stations(line_tfl_id=line.tfl_id, use_cache=False, skip_database_validation=True)

            # Validate that stations were populated
            station_count_result = await self.db.execute(select(func.count()).select_from(Station))
            station_count = station_count_result.scalar_one()

            if station_count == 0:
                logger.error("no_stations_found_after_fetch")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="No stations found after fetching from TfL API. Cannot build graph.",
                )

            logger.info("stations_validated", station_count=station_count)

            # Soft delete existing connections (within transaction - will rollback if building fails)
            # Use soft delete instead of hard delete to eliminate 503 window during rebuild.
            # Old connections remain visible until new connections are committed (issue #230).
            # Note: StationConnection.id.isnot(None) is always true - this is intentional bulk soft-delete
            await soft_delete(self.db, StationConnection, StationConnection.id.isnot(None))
            # No flush() needed! Partial unique index (WHERE deleted_at IS NULL) allows
            # soft-deleted and new records to coexist until commit. This enables atomic
            # transition with zero downtime.
            logger.info("existing_connections_soft_deleted")

            stations_set: set[str] = set()
            pending_connections: set[tuple[uuid.UUID, uuid.UUID, uuid.UUID]] = set()
            connections_count = 0

            # Process each line
            for line in lines:
                logger.info("processing_line_for_graph", line_name=line.name, line_tfl_id=line.tfl_id)

                # Process both directions and collect route data
                # Note: Duplicate connections are prevented by pending_connections set
                # Even if inbound and outbound routes overlap, we won't create duplicates
                inbound_route_data = None
                outbound_route_data = None

                for direction in ["inbound", "outbound"]:
                    conn_count, route_data = await self._process_route_sequence(
                        line,
                        direction,
                        stations_set,
                        pending_connections,
                    )
                    connections_count += conn_count

                    # Store route data for later processing
                    if direction == "inbound":
                        inbound_route_data = route_data
                    else:
                        outbound_route_data = route_data

                # Extract and store route sequences for this line
                self._store_line_routes(line, inbound_route_data, outbound_route_data)

            # Commit all changes (delete + new connections) atomically
            # If we reach here, everything succeeded
            await self.db.commit()

            # Invalidate all station and line caches since we've rebuilt the graph
            # This ensures subsequent API calls get fresh data from the database
            await self.cache.delete("stations:all")
            for line in lines:
                await self.cache.delete(f"stations:line:{line.tfl_id}")
            # Also clear lines cache in case metadata changed
            await self.cache.delete("lines")
            logger.info("invalidated_all_tfl_caches", lines_invalidated=len(lines))

            # Count stations with hub NaPTAN codes (interchange stations)
            hubs_count_result = await self.db.execute(
                select(func.count()).select_from(Station).where(Station.hub_naptan_code.isnot(None))
            )
            hubs_count = hubs_count_result.scalar_one()

            build_result = {
                "lines_count": len(lines),
                "stations_count": len(stations_set),
                "connections_count": connections_count,
                "hubs_count": hubs_count,
            }

            logger.info("building_station_graph_complete", **build_result)
            return build_result

        except HTTPException:
            # Re-raise HTTP exceptions (validation failures)
            await self.db.rollback()
            raise
        except Exception as e:
            logger.error("build_station_graph_failed", error=str(e), exc_info=e)
            await self.db.rollback()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to build station graph.",
            ) from e

    async def get_network_graph(self) -> dict[str, list[NetworkConnection]]:
        """
        Get the station network graph as an adjacency list for GUI route building.

        Returns a mapping of station TfL IDs to their connected stations with line information.
        This helps the GUI constrain user choices to valid next stations.

        Returns:
            Dictionary mapping station_tfl_id to list of connected stations:
            {
                "station_tfl_id": [
                    {
                        "station_id": UUID,
                        "station_tfl_id": str,
                        "station_name": str,
                        "line_id": UUID,
                        "line_tfl_id": str,
                        "line_name": str,
                    },
                    ...
                ]
            }

        Raises:
            HTTPException: 503 if graph hasn't been built yet
        """
        logger.info("fetching_network_graph")

        try:
            # Check if graph exists (filter out soft-deleted connections)
            query = select(StationConnection).limit(1)
            query = add_active_filter(query, StationConnection)
            result = await self.db.execute(query)
            if not result.scalar_one_or_none():
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Station graph has not been built yet. Please contact administrator.",
                )

            # Get all connections with from_station, to_station, and line data using aliased joins
            # Filter out soft-deleted connections
            from_station_alias = aliased(Station)
            to_station_alias = aliased(Station)

            base_query = (
                select(StationConnection, from_station_alias, to_station_alias, Line)
                .join(from_station_alias, from_station_alias.id == StationConnection.from_station_id)
                .join(to_station_alias, to_station_alias.id == StationConnection.to_station_id)
                .join(Line, Line.id == StationConnection.line_id)
            )
            filtered_query = add_active_filter(base_query, StationConnection)
            result = await self.db.execute(filtered_query)
            connections = result.all()

            # Build adjacency list
            graph: dict[str, list[NetworkConnection]] = {}

            for connection_row in connections:
                # Access aliased stations using index positions (StationConnection, FromStation, ToStation, Line)
                from_station = connection_row[1]
                to_station = connection_row[2]
                line = connection_row[3]

                # Initialize list if needed
                if from_station.tfl_id not in graph:
                    graph[from_station.tfl_id] = []

                # Add connection
                graph[from_station.tfl_id].append(
                    NetworkConnection(
                        station_id=str(to_station.id),
                        station_tfl_id=to_station.tfl_id,
                        station_name=to_station.name,
                        line_id=str(line.id),
                        line_tfl_id=line.tfl_id,
                        line_name=line.name,
                    )
                )

            logger.info("network_graph_fetched", stations_count=len(graph))
            return graph

        except HTTPException:
            raise
        except Exception as e:
            logger.error("get_network_graph_failed", error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to fetch network graph.",
            ) from e

    async def get_station_by_tfl_id(self, tfl_id: str) -> Station:
        """
        Get station from database by TfL ID.

        Args:
            tfl_id: TfL station ID (e.g., '940GZZLUOXC' for Oxford Circus)

        Returns:
            Station object from database

        Raises:
            HTTPException(404): If station not found in database
        """
        result = await self.db.execute(select(Station).where(Station.tfl_id == tfl_id))
        station = result.scalar_one_or_none()

        if station is None:
            logger.warning("station_not_found_by_tfl_id", tfl_id=tfl_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=(
                    f"Station with TfL ID '{tfl_id}' not found. "
                    "Please ensure TfL data is imported via /admin/tfl/build-graph endpoint."
                ),
            )

        return station

    async def resolve_station_or_hub(self, tfl_id_or_hub: str, line_tfl_id: str | None = None) -> Station:
        """
        Resolve station TfL ID or hub NaPTAN code to a Station.

        This method supports both station TfL IDs (e.g., '940GZZLUSVS') and hub NaPTAN
        codes (e.g., 'HUBSVS') for improved UX. When a hub code is provided with a
        line context, it selects the station in that hub that serves the specified line.

        Resolution logic (using pure helper functions from app.helpers.station_resolution):
        1. Query for stations matching tfl_id OR hub_naptan_code (single query)
        2. If direct tfl_id match found, use it (backward compatibility)
        3. Otherwise, treat all results as hub stations
        4. If hub found and line_tfl_id provided, filter stations that serve that line
        5. If hub found and line_tfl_id is None (destination segment), return first station

        Args:
            tfl_id_or_hub: Either a station TfL ID or a hub NaPTAN code
            line_tfl_id: Optional line TfL ID for line-context resolution

        Returns:
            Station object from database

        Raises:
            HTTPException(404): If station/hub not found or no station serves line

        Examples:
            # Station ID (backward compatible)
            >>> await resolve_station_or_hub("940GZZLUSVS", "victoria")
            <Station(tfl_id='940GZZLUSVS', name='Seven Sisters Underground')>

            # Hub code with line context
            >>> await resolve_station_or_hub("HUBSVS", "victoria")
            <Station(tfl_id='940GZZLUSVS', name='Seven Sisters Underground')>

            # Hub code as destination (no line)
            >>> await resolve_station_or_hub("HUBSVS", None)
            <Station(tfl_id='910GSEVNSIS', ...)>  # Returns any station in hub
        """
        # Single query: check both tfl_id and hub_naptan_code (optimization from code review)
        result = await self.db.execute(
            select(Station).where(
                or_(
                    Station.tfl_id == tfl_id_or_hub,
                    Station.hub_naptan_code == tfl_id_or_hub,
                )
            )
        )
        stations = list(result.scalars().all())

        if not stations:
            logger.warning(
                "station_or_hub_not_found",
                tfl_id_or_hub=tfl_id_or_hub,
                line_tfl_id=line_tfl_id,
            )
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=str(StationNotFoundError(tfl_id_or_hub)),
            )

        # Check if we have a direct tfl_id match (takes precedence for backward compatibility)
        direct_match = next((s for s in stations if s.tfl_id == tfl_id_or_hub), None)
        if direct_match is not None:
            logger.debug(
                "station_resolved_by_tfl_id",
                tfl_id=tfl_id_or_hub,
                station_name=direct_match.name,
                line_tfl_id=line_tfl_id,
            )
            return direct_match

        # All remaining stations are hub matches (hub_naptan_code == tfl_id_or_hub)
        hub_stations = stations

        # Filter by line context if provided (using pure helper function)
        if line_tfl_id is not None:
            matching_stations = filter_stations_by_line(hub_stations, line_tfl_id)

            if not matching_stations:
                logger.warning(
                    "hub_no_station_serves_line",
                    hub_code=tfl_id_or_hub,
                    line_tfl_id=line_tfl_id,
                    hub_stations=[s.tfl_id for s in hub_stations],
                )
                error = NoMatchingStationsError(
                    tfl_id_or_hub,
                    line_tfl_id,
                    [s.tfl_id for s in hub_stations],
                )
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=str(error),
                )

            # Use pure function to select station (deterministic)
            if len(matching_stations) > 1:
                logger.info(
                    "hub_multiple_stations_serve_line",
                    hub_code=tfl_id_or_hub,
                    line_tfl_id=line_tfl_id,
                    matching_stations=[s.tfl_id for s in matching_stations],
                )

            resolved_station = select_station_from_candidates(matching_stations)

            logger.info(
                "hub_resolved_with_line_context",
                hub_code=tfl_id_or_hub,
                line_tfl_id=line_tfl_id,
                resolved_tfl_id=resolved_station.tfl_id,
                resolved_name=resolved_station.name,
            )
            return resolved_station

        # No line context (destination segment) - use pure helper function
        resolved_station = select_station_from_candidates(hub_stations)
        logger.info(
            "hub_resolved_without_line_context",
            hub_code=tfl_id_or_hub,
            resolved_tfl_id=resolved_station.tfl_id,
            resolved_name=resolved_station.name,
            hub_stations=[s.tfl_id for s in hub_stations],
        )
        return resolved_station

    async def get_line_by_tfl_id(self, tfl_id: str) -> Line:
        """
        Get line from database by TfL ID.

        Args:
            tfl_id: TfL line ID (e.g., 'victoria', 'northern', 'central')

        Returns:
            Line object from database

        Raises:
            HTTPException(404): If line not found in database
        """
        result = await self.db.execute(select(Line).where(Line.tfl_id == tfl_id))
        line = result.scalar_one_or_none()

        if line is None:
            logger.warning("line_not_found_by_tfl_id", tfl_id=tfl_id)
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=(
                    f"Line with TfL ID '{tfl_id}' not found. "
                    "Please ensure TfL data is imported via /admin/tfl/build-graph endpoint."
                ),
            )

        return line

    def _validate_route_segment_count(self, segments: list[RouteSegmentRequest]) -> tuple[bool, str | None]:
        """
        Validate route has appropriate number of segments.

        Args:
            segments: List of route segments to validate

        Returns:
            Tuple of (is_valid, error_message_or_none)
        """
        # Check minimum segments
        if len(segments) < MIN_ROUTE_SEGMENTS:
            return False, f"Route must have at least {MIN_ROUTE_SEGMENTS} segments (start and end)."

        # Check maximum segments
        if len(segments) > MAX_ROUTE_SEGMENTS:
            return (
                False,
                f"Route cannot have more than {MAX_ROUTE_SEGMENTS} segments. "
                f"Current route has {len(segments)} segments.",
            )

        return True, None

    async def _validate_route_acyclic(self, segments: list[RouteSegmentRequest]) -> tuple[bool, str | None, int | None]:
        """
        Validate route doesn't visit same station twice.

        Args:
            segments: List of route segments to validate

        Returns:
            Tuple of (is_valid, error_message_or_none, duplicate_index_or_none)
        """
        station_tfl_ids = [segment.station_tfl_id for segment in segments]
        unique_stations = set(station_tfl_ids)

        if len(unique_stations) != len(station_tfl_ids):
            # Find the duplicate station
            seen: set[str] = set()
            for idx, station_tfl_id in enumerate(station_tfl_ids):
                if station_tfl_id in seen:
                    # Get station name for error message
                    station = await self.get_station_by_tfl_id(station_tfl_id)
                    station_name = station.name if station else "Unknown"

                    logger.warning(
                        "route_validation_failed_duplicate_station",
                        station_tfl_id=station_tfl_id,
                        station_name=station_name,
                        segment_index=idx,
                    )

                    return (
                        False,
                        f"Route cannot visit the same station ('{station_name}') more than once. "
                        f"Duplicate found at segment {idx + 1}.",
                        idx,
                    )
                seen.add(station_tfl_id)

        return True, None, None

    def _validate_intermediate_line_ids(
        self, segments: list[RouteSegmentRequest]
    ) -> tuple[bool, str | None, int | None]:
        """
        Validate only final segment has NULL line_tfl_id.

        Args:
            segments: List of route segments to validate

        Returns:
            Tuple of (is_valid, error_message_or_none, invalid_index_or_none)
        """
        # Validate that only the final segment can have NULL line_tfl_id
        # All intermediate segments (0 to len-2) must have a line to travel on
        for i in range(len(segments) - 1):
            if segments[i].line_tfl_id is None:
                logger.warning(
                    "route_validation_failed_null_intermediate_line",
                    segment_index=i,
                    station_tfl_id=segments[i].station_tfl_id,
                )
                return (
                    False,
                    f"Segment {i} must have a line_tfl_id. "
                    "Only the final segment (destination) can have NULL line_tfl_id.",
                    i,
                )

        return True, None, None

    async def _fetch_route_validation_data(
        self, segments: list[RouteSegmentRequest]
    ) -> tuple[dict[str, Station], dict[str, Line], dict[str, list[Station]]]:
        """
        Bulk fetch all data needed for route validation.

        Fetches:
        - All stations referenced in segments
        - All lines referenced in segments
        - All hub-equivalent stations

        Args:
            segments: List of route segments to fetch data for

        Returns:
            Tuple of (stations_map, lines_map, hub_map)
        """
        # Bulk fetch all stations and lines to avoid redundant lookups
        # Filter out None values from line_tfl_ids (destination segments have no line)
        unique_line_ids = {seg.line_tfl_id for seg in segments if seg.line_tfl_id is not None}

        # Fetch all stations - use resolve_station_or_hub to support both station IDs and hub codes
        # We need to pair station IDs with their line contexts from the segments
        station_line_pairs = [(seg.station_tfl_id, seg.line_tfl_id) for seg in segments]

        stations_map = {}
        for tfl_id_or_hub, line_context in station_line_pairs:
            if tfl_id_or_hub not in stations_map:
                # resolve_station_or_hub supports both station TfL IDs and hub codes
                station = await self.resolve_station_or_hub(tfl_id_or_hub, line_context)
                stations_map[tfl_id_or_hub] = station

                # Cache by station tfl_id to avoid redundant resolutions (code review optimization)
                # If later segments reference same station by its tfl_id, reuse this result
                # NOTE: We don't cache by hub_naptan_code because hub resolution is line-context
                # dependent - "HUBTEST" with line1 vs line2 may resolve to different stations
                if station.tfl_id != tfl_id_or_hub:
                    stations_map[station.tfl_id] = station

        # Fetch all lines
        lines_map = {}
        for tfl_id in unique_line_ids:
            line = await self.get_line_by_tfl_id(tfl_id)
            lines_map[tfl_id] = line

        # Bulk fetch all hub-equivalent stations to avoid per-segment queries
        # Collect unique hub codes from fetched stations
        unique_hub_codes = {
            station.hub_naptan_code for station in stations_map.values() if station.hub_naptan_code is not None
        }

        # Fetch all stations with these hub codes in one query
        hub_map: dict[str, list[Station]] = {}
        if unique_hub_codes:
            result = await self.db.execute(
                select(Station).where(
                    Station.hub_naptan_code.in_(unique_hub_codes),
                    Station.deleted_at.is_(None),
                )
            )
            hub_stations = list(result.scalars().all())

            # Build hub_naptan_code -> [stations] map
            for station in hub_stations:
                if station.hub_naptan_code:
                    if station.hub_naptan_code not in hub_map:
                        hub_map[station.hub_naptan_code] = []
                    hub_map[station.hub_naptan_code].append(station)

        return stations_map, lines_map, hub_map

    def _format_connection_error_message(
        self,
        from_station: Station,
        to_station: Station,
        line: Line,
    ) -> str:
        """
        Format error message for connection validation failure.

        Generates different messages based on whether stations share the line
        (different branches) vs completely different lines.

        Args:
            from_station: Starting station
            to_station: Destination station
            line: Line being traveled on

        Returns:
            Formatted error message
        """
        # Check if both stations serve the same line (different branches scenario)
        from_lines = set(from_station.lines)
        to_lines = set(to_station.lines)
        common_lines = from_lines & to_lines

        if line.tfl_id in common_lines:
            # Stations are on the same line but different branches
            return (
                f"'{from_station.name}' and '{to_station.name}' are both on the "
                f"{line.name} line, but they are on different branches that don't connect directly. "
                f"You may need to select intermediate stations or change lines at an interchange."
            )
        # Stations are on different lines or connection doesn't exist
        return f"No connection found between '{from_station.name}' and '{to_station.name}' on {line.name} line."

    async def _validate_segment_connections(
        self,
        segments: list[RouteSegmentRequest],
        stations_map: dict[str, Station],
        lines_map: dict[str, Line],
        hub_map: dict[str, list[Station]],
    ) -> tuple[bool, str, int | None]:
        """
        Validate all segment connections in route.

        Args:
            segments: Route segments to validate
            stations_map: Pre-fetched station objects
            lines_map: Pre-fetched line objects
            hub_map: Pre-fetched hub-equivalent stations

        Returns:
            Tuple of (is_valid, message, invalid_segment_index_or_none)
        """
        # Validate each segment connection using cached data
        for i in range(len(segments) - 1):
            current_segment = segments[i]
            next_segment = segments[i + 1]

            # Use cached station and line objects
            from_station = stations_map[current_segment.station_tfl_id]
            to_station = stations_map[next_segment.station_tfl_id]
            # We already validated that intermediate segments have non-null line_tfl_id
            assert current_segment.line_tfl_id is not None  # For type checker
            line = lines_map[current_segment.line_tfl_id]

            # Check if connection exists (with hub interchange support)
            # Hub stations are equivalent - try all combinations of hub-equivalent stations
            is_connected, _connected_from, _connected_to = await self._check_any_hub_connection(
                from_station=from_station,
                to_station=to_station,
                line=line,
                segment_index=i,
                hub_map=hub_map,
            )

            if not is_connected:
                # Format error message using helper
                message = self._format_connection_error_message(from_station, to_station, line)

                logger.warning(
                    "route_validation_failed",
                    segment_index=i,
                    from_station_tfl_id=current_segment.station_tfl_id,
                    to_station_tfl_id=next_segment.station_tfl_id,
                    line_tfl_id=current_segment.line_tfl_id,
                    from_station_name=from_station.name,
                    to_station_name=to_station.name,
                )

                return False, message, i

        return True, "Route is valid.", None

    async def validate_route(self, segments: list[RouteSegmentRequest]) -> tuple[bool, str, int | None]:
        """
        Validate a route by checking if connections exist between segments.

        Validates routes by checking that consecutive stations exist in the same route sequence
        for the specified line, with support for hub interchanges. Also enforces acyclic routes
        (no duplicate stations) and maximum segment limits.

        Args:
            segments: List of route segments (station + line pairs)

        Returns:
            Tuple of (is_valid, message, invalid_segment_index)
        """
        # Input validation - segment count
        is_valid, error_msg = self._validate_route_segment_count(segments)
        if not is_valid:
            assert error_msg is not None  # Type narrowing: error_msg is str when is_valid is False
            return False, error_msg, None

        # Input validation - acyclic route (no duplicate stations)
        is_valid, error_msg, idx = await self._validate_route_acyclic(segments)
        if not is_valid:
            assert error_msg is not None  # Type narrowing: error_msg is str when is_valid is False
            return False, error_msg, idx

        # Input validation - intermediate segments must have line IDs
        is_valid, error_msg, idx = self._validate_intermediate_line_ids(segments)
        if not is_valid:
            assert error_msg is not None  # Type narrowing: error_msg is str when is_valid is False
            return False, error_msg, idx

        logger.info("validating_route", segments_count=len(segments))

        try:
            # Bulk fetch all required data
            stations_map, lines_map, hub_map = await self._fetch_route_validation_data(segments)

            # Validate segment connections
            is_valid, message, invalid_idx = await self._validate_segment_connections(
                segments, stations_map, lines_map, hub_map
            )

            if is_valid:
                logger.info("route_validation_successful", segments_count=len(segments))

            return is_valid, message, invalid_idx

        except HTTPException:
            # Re-raise HTTP exceptions (e.g., 404 from get_station_by_tfl_id)
            raise
        except Exception as e:
            logger.error("validate_route_failed", error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to validate route.",
            ) from e

    def _get_hub_equivalent_stations(self, station: Station, hub_map: dict[str, list[Station]]) -> list[Station]:
        """
        Get all stations equivalent to this station via hub interchange.

        Stations are equivalent if they share the same hub_naptan_code.
        This allows routing through any station in the same hub/interchange.

        Args:
            station: Station to find equivalents for
            hub_map: Pre-fetched map of hub_naptan_code -> [stations]

        Returns:
            List of equivalent stations (always includes original station)

        Examples:
            Seven Sisters Rail (hub='HUBSVS') returns:
            - [Seven Sisters Rail, Seven Sisters Tube]

            Regular station (no hub) returns:
            - [Station itself]
        """
        if not station.hub_naptan_code:
            # No hub - station only equivalent to itself
            return [station]

        # Look up hub equivalents from pre-fetched map
        hub_stations = hub_map.get(station.hub_naptan_code, [])

        # Return a copy to prevent unintended modifications to hub_map by callers
        # Use explicit if-else instead of `or` operator because empty list is falsy:
        # - hub_stations or [station] would incorrectly return [station] for empty list
        # - We want to return [] if hub code exists but has no stations in map
        return list(hub_stations) if hub_stations else [station]

    async def _check_any_hub_connection(
        self,
        from_station: Station,
        to_station: Station,
        line: Line,
        segment_index: int,
        hub_map: dict[str, list[Station]],
    ) -> tuple[bool, Station | None, Station | None]:
        """
        Check if any hub-equivalent station pair has a valid connection on the line.

        Hub stations are interchangeable/equivalent for routing purposes. This method
        tries all combinations of hub-equivalent stations to find a valid connection.

        Args:
            from_station: User-specified starting station
            to_station: User-specified destination station
            line: Line to travel on
            segment_index: Current segment index (for logging)
            hub_map: Pre-fetched map of hub_naptan_code -> [stations]

        Returns:
            Tuple of (is_connected, actual_from_station, actual_to_station)
            - is_connected: True if any combination connects
            - actual_from_station: Station that actually connects (may differ from user's choice)
            - actual_to_station: Station that actually connects (may differ from user's choice)

        Examples:
            User specifies: Bush Hill Park  Seven Sisters Rail (Victoria line)
            - Seven Sisters Rail not on Victoria line
            - Seven Sisters Tube IS on Victoria line (same hub='HUBSVS')
            - Returns: (True, bush_hill_park, seven_sisters_tube)
        """
        # Get all hub-equivalent stations for both endpoints from pre-fetched map
        from_stations = self._get_hub_equivalent_stations(from_station, hub_map)
        to_stations = self._get_hub_equivalent_stations(to_station, hub_map)

        # Try all combinations of hub-equivalent stations
        for from_st in from_stations:
            for to_st in to_stations:
                is_connected = await self._check_connection(
                    from_station_id=from_st.id,
                    to_station_id=to_st.id,
                    line_id=line.id,
                )

                if is_connected:
                    # Log hub interchange if we used different stations than user specified
                    if from_st.id != from_station.id or to_st.id != to_station.id:
                        logger.info(
                            "hub_interchange_detected",
                            segment_index=segment_index,
                            user_from_station=from_station.name,
                            user_from_tfl_id=from_station.tfl_id,
                            actual_from_station=from_st.name,
                            actual_from_tfl_id=from_st.tfl_id,
                            user_to_station=to_station.name,
                            user_to_tfl_id=to_station.tfl_id,
                            actual_to_station=to_st.name,
                            actual_to_tfl_id=to_st.tfl_id,
                            hub_naptan_code=from_station.hub_naptan_code or to_station.hub_naptan_code,
                            line_tfl_id=line.tfl_id,
                        )

                    return True, from_st, to_st

        # No combination connects
        return False, None, None

    async def _check_connection(
        self,
        from_station_id: uuid.UUID,
        to_station_id: uuid.UUID,
        line_id: uuid.UUID,
    ) -> bool:
        """
        Check if two stations are reachable on the same route sequence in the correct direction.

        This validates that:
        1. Both stations exist in at least one route sequence for the specified line
        2. The stations appear in the correct order (directional validation)

        This prevents both cross-branch travel (e.g., Bank  Charing Cross on Northern line)
        and backwards travel (e.g., Piccadilly Circus  Arsenal on Piccadilly line when the
        route sequence goes Arsenal  Piccadilly Circus).

        Args:
            from_station_id: Starting station UUID
            to_station_id: Destination station UUID
            line_id: Line UUID

        Returns:
            True if both stations exist in the same route sequence in the correct order,
            False otherwise
        """
        # Get station and line objects
        from_station = await self.db.get(Station, from_station_id)
        to_station = await self.db.get(Station, to_station_id)
        line = await self.db.get(Line, line_id)

        if not from_station or not to_station or not line:
            logger.warning(
                "check_connection_missing_entity",
                from_station_id=from_station_id,
                to_station_id=to_station_id,
                line_id=line_id,
            )
            return False

        # Check if route sequences exist for this line
        if not line.route_variants or "routes" not in line.route_variants:
            logger.warning(
                "check_connection_no_routes",
                line_tfl_id=line.tfl_id,
                line_name=line.name,
            )
            return False

        # Use pure helper function to find valid connection
        # Performance: O(r * n) where r = route variants (2-6), n = stations (20-60)
        # With infrequent validation (only during route creation/editing), performance is negligible.
        routes = line.route_variants["routes"]
        result = find_valid_connection_in_routes(
            from_station.tfl_id,
            to_station.tfl_id,
            routes,
        )

        if result and result["found"]:
            logger.debug(
                "connection_found_in_route",
                route_name=result["route_name"],
                direction=result["direction"],
                from_station=from_station.name,
                to_station=to_station.name,
                from_index=result["from_index"],
                to_index=result["to_index"],
            )
            return True

        # Connection not found - check if it's backwards travel or different branches
        # If result has indices but found=False, stations exist but in wrong order (backwards)
        # If result is None or has no indices, stations not in same route (different branches)
        if result and result["from_index"] is not None and result["to_index"] is not None:
            # Backwards travel detected
            logger.debug(
                "connection_found_but_wrong_direction",
                route_name=result["route_name"],
                direction=result["direction"],
                from_station=from_station.name,
                to_station=to_station.name,
                from_index=result["from_index"],
                to_index=result["to_index"],
                line_name=line.name,
            )
        else:
            # Stations not in any common route sequence (different branches)
            logger.debug(
                "connection_not_found_different_branches",
                from_station=from_station.name,
                to_station=to_station.name,
                line_name=line.name,
            )
        return False

    async def get_line_routes(self, line_tfl_id: str) -> LineRoutesResponse:
        """
        Get route variants for a specific line.

        Args:
            line_tfl_id: TfL line ID (e.g., "victoria", "elizabeth-line")

        Returns:
            Line routes data (line_tfl_id and list of route variants)

        Raises:
            HTTPException: 404 if line not found, 503 if routes haven't been built yet
        """
        logger.info("fetching_line_routes", line_tfl_id=line_tfl_id)

        try:
            # Look up line by TfL ID
            result = await self.db.execute(select(Line).where(Line.tfl_id == line_tfl_id))
            line = result.scalar_one_or_none()

            if not line:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Line '{line_tfl_id}' not found.",
                )

            # Check if routes have been built (None means not built, {} means no routes found)
            if line.route_variants is None:
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Route data has not been built yet. Please contact administrator.",
                )

            # Return line routes data
            routes_data = line.route_variants.get("routes", [])
            route_variants = [RouteVariant(**route) for route in routes_data]
            return LineRoutesResponse(
                line_tfl_id=line.tfl_id,
                routes=route_variants,
            )

        except HTTPException:
            raise
        except Exception as e:
            logger.error("get_line_routes_failed", line_tfl_id=line_tfl_id, error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to fetch routes for line '{line_tfl_id}'.",
            ) from e

    async def get_station_routes(self, station_tfl_id: str) -> StationRoutesResponse:
        """
        Get all routes passing through a specific station.

        Args:
            station_tfl_id: TfL station ID (e.g., "940GZZLUVIC")

        Returns:
            Station routes data (station_tfl_id, station_name, and list of routes)

        Raises:
            HTTPException: 404 if station not found, 503 if routes haven't been built yet
        """
        logger.info("fetching_station_routes", station_tfl_id=station_tfl_id)

        try:
            # Look up station by TfL ID
            result = await self.db.execute(select(Station).where(Station.tfl_id == station_tfl_id))
            station = result.scalar_one_or_none()

            if not station:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Station '{station_tfl_id}' not found.",
                )

            # Get all lines that serve this station
            line_tfl_ids = station.lines
            if not line_tfl_ids:
                # Station exists but has no lines (shouldn't happen, but handle gracefully)
                return StationRoutesResponse(
                    station_tfl_id=station.tfl_id,
                    station_name=station.name,
                    routes=[],
                )

            # Fetch all lines that serve this station
            lines_result = await self.db.execute(select(Line).where(Line.tfl_id.in_(line_tfl_ids)))
            lines: list[Line] = list(lines_result.scalars().all())

            # Collect routes that pass through this station
            station_routes: list[StationRouteInfo] = []
            for line in lines:
                if line.route_variants is None:
                    continue

                routes = line.route_variants.get("routes", [])
                station_routes.extend(
                    StationRouteInfo(
                        line_tfl_id=line.tfl_id,
                        line_name=line.name,
                        route_name=route["name"],
                        service_type=route["service_type"],
                        direction=route["direction"],
                    )
                    for route in routes
                    if station_tfl_id in route["stations"]
                )

            # Check if any routes were found
            if not station_routes and lines:
                # Lines exist but no routes built yet
                raise HTTPException(
                    status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                    detail="Route data has not been built yet. Please contact administrator.",
                )

            return StationRoutesResponse(
                station_tfl_id=station.tfl_id,
                station_name=station.name,
                routes=station_routes,
            )

        except HTTPException:
            raise
        except Exception as e:
            logger.error("get_station_routes_failed", station_tfl_id=station_tfl_id, error=str(e), exc_info=e)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to fetch routes for station '{station_tfl_id}'.",
            ) from e
